{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXZK2Pm8oA7IpwDKYM11gM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahimku2020/fahimku2020/blob/main/Best_pmi_keywords_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pv-Vq6cTx5m",
        "outputId": "b4d02ada-50af-4765-f1a2-3b7697dd115a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=1007657a5857629d0305c065ce7215e868953c518165e0f6f1e0d45efa8da212\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
            "\u001b[0mERROR: unknown command \"i\"\n"
          ]
        }
      ],
      "source": [
        "!pip  install wikipedia\n",
        "!pip install nltk\n",
        "!pip install re\n",
        "!pip i stall math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import nltk\n",
        "import wikipedia\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def fetch_wikipedia_content(topic):\n",
        "    \"\"\"\n",
        "    Fetch Wikipedia content for a given topic\n",
        "\n",
        "    Args:\n",
        "        topic (str): Wikipedia topic to search\n",
        "\n",
        "    Returns:\n",
        "        str: Wikipedia page content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        page = wikipedia.page(topic)\n",
        "        return page.content\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        print(f\"Multiple pages found. Choosing the first one: {e.options[0]}\")\n",
        "        page = wikipedia.page(e.options[0])\n",
        "        return page.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching Wikipedia content: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess text by removing stopwords, special characters, digits,\n",
        "    years, months, dates, and punctuations\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to preprocess\n",
        "\n",
        "    Returns:\n",
        "        list: Preprocessed tokens\n",
        "    \"\"\"\n",
        "    # Remove special characters, digits, punctuations\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def extract_bigram_keywords(tokens, top_n=20):\n",
        "    \"\"\"\n",
        "    Extract top bigram keywords using likelihood ratio\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Preprocessed tokens\n",
        "        top_n (int): Number of top bigrams to extract\n",
        "\n",
        "    Returns:\n",
        "        list: Top bigram keywords with their likelihood ratios\n",
        "    \"\"\"\n",
        "    # Create bigram collocation finder\n",
        "    finder = BigramCollocationFinder.from_words(tokens)\n",
        "\n",
        "    # Filter bigrams that appear at least 3 times\n",
        "    finder.apply_freq_filter(4)\n",
        "\n",
        "    # Calculate likelihood ratio for bigrams\n",
        "    bigram_measures = BigramAssocMeasures()\n",
        "    bigram_scores = finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
        "\n",
        "    # Sort and return top N bigrams\n",
        "    top_bigrams = sorted(bigram_scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    return top_bigrams\n",
        "\n",
        "def main(topic):\n",
        "    \"\"\"\n",
        "    Main function to extract PMI bigram keywords from Wikipedia\n",
        "\n",
        "    Args:\n",
        "        topic (str): Wikipedia topic to analyze\n",
        "    \"\"\"\n",
        "    # Fetch Wikipedia content\n",
        "    content = fetch_wikipedia_content(topic)\n",
        "\n",
        "    # Split into sentences\n",
        "    sentences = sent_tokenize(content)\n",
        "\n",
        "    # Preprocess all tokens from sentences\n",
        "    all_tokens = []\n",
        "    for sentence in sentences:\n",
        "        all_tokens.extend(preprocess_text(sentence))\n",
        "\n",
        "    # Extract top bigram keywords\n",
        "    top_bigrams = extract_bigram_keywords(all_tokens)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Top Bigram Keywords for '{topic}':\")\n",
        "    for bigram, score in top_bigrams:\n",
        "        print(f\"Bigram: {' '.join(bigram)}, Likelihood Ratio: {score:.4f}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example topics to try\n",
        "    topics = [\"Amitabh Bachan\", \"Artificial Intelligence\",\"computer science\"]\n",
        "\n",
        "    for topic in topics:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        main(topic)\n",
        "        print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogzXfwiETyyy",
        "outputId": "b7cd20eb-7a3e-4f1d-d3ea-25ca294843be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Top Bigram Keywords for 'Amitabh Bachan':\n",
            "Bigram: box office, Likelihood Ratio: 276.1144\n",
            "Bigram: best actor, Likelihood Ratio: 185.1788\n",
            "Bigram: amitabh bachchan, Likelihood Ratio: 126.2224\n",
            "Bigram: award best, Likelihood Ratio: 110.8498\n",
            "Bigram: angry young, Likelihood Ratio: 91.6936\n",
            "Bigram: young man, Likelihood Ratio: 82.1616\n",
            "Bigram: salim khan, Likelihood Ratio: 69.2133\n",
            "Bigram: kaala patthar, Likelihood Ratio: 63.8573\n",
            "Bigram: harivansh rai, Likelihood Ratio: 58.8533\n",
            "Bigram: ka sikandar, Likelihood Ratio: 58.8533\n",
            "Bigram: muqaddar ka, Likelihood Ratio: 58.8533\n",
            "Bigram: prakash mehra, Likelihood Ratio: 56.2192\n",
            "Bigram: donated million, Likelihood Ratio: 52.7670\n",
            "Bigram: yash chopra, Likelihood Ratio: 49.2940\n",
            "Bigram: national film, Likelihood Ratio: 48.0947\n",
            "Bigram: filmfare award, Likelihood Ratio: 42.9489\n",
            "Bigram: filmfare best, Likelihood Ratio: 42.9489\n",
            "Bigram: bachchans performance, Likelihood Ratio: 34.4926\n",
            "Bigram: action drama, Likelihood Ratio: 33.6735\n",
            "Bigram: international film, Likelihood Ratio: 28.0616\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "Top Bigram Keywords for 'Artificial Intelligence':\n",
            "Bigram: artificial intelligence, Likelihood Ratio: 313.2809\n",
            "Bigram: neural networks, Likelihood Ratio: 226.9099\n",
            "Bigram: machine learning, Likelihood Ratio: 155.1537\n",
            "Bigram: bad actors, Likelihood Ratio: 106.4822\n",
            "Bigram: data centers, Likelihood Ratio: 100.6497\n",
            "Bigram: deep learning, Likelihood Ratio: 93.5083\n",
            "Bigram: nuclear power, Likelihood Ratio: 77.8757\n",
            "Bigram: natural language, Likelihood Ratio: 77.1181\n",
            "Bigram: virtual assistants, Likelihood Ratio: 68.7701\n",
            "Bigram: general intelligence, Likelihood Ratio: 68.7196\n",
            "Bigram: existential risk, Likelihood Ratio: 64.6352\n",
            "Bigram: widely used, Likelihood Ratio: 64.2962\n",
            "Bigram: take actions, Likelihood Ratio: 61.4944\n",
            "Bigram: local search, Likelihood Ratio: 60.6972\n",
            "Bigram: inputs outputs, Likelihood Ratio: 54.2061\n",
            "Bigram: knowledge representation, Likelihood Ratio: 52.8725\n",
            "Bigram: ai research, Likelihood Ratio: 51.8110\n",
            "Bigram: reinforcement learning, Likelihood Ratio: 49.4858\n",
            "Bigram: autonomous vehicles, Likelihood Ratio: 49.3473\n",
            "Bigram: speech recognition, Likelihood Ratio: 48.7692\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "Top Bigram Keywords for 'computer science':\n",
            "Bigram: computer science, Likelihood Ratio: 313.1716\n",
            "Bigram: artificial intelligence, Likelihood Ratio: 87.0062\n",
            "Bigram: formal methods, Likelihood Ratio: 72.5919\n",
            "Bigram: software engineering, Likelihood Ratio: 70.1179\n",
            "Bigram: mechanical calculator, Likelihood Ratio: 55.5780\n",
            "Bigram: humancomputer interaction, Likelihood Ratio: 54.6439\n",
            "Bigram: analytical engine, Likelihood Ratio: 47.0073\n",
            "Bigram: data structures, Likelihood Ratio: 37.3231\n",
            "Bigram: theory computation, Likelihood Ratio: 35.9296\n",
            "Bigram: programming paradigm, Likelihood Ratio: 29.6771\n",
            "Bigram: computer graphics, Likelihood Ratio: 25.4312\n",
            "Bigram: programming programming, Likelihood Ratio: 18.9759\n",
            "Bigram: theoretical computer, Likelihood Ratio: 18.3856\n",
            "Bigram: computer programs, Likelihood Ratio: 14.6750\n",
            "Bigram: computer systems, Likelihood Ratio: 5.7697\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oRHVnwcoe5WX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IzBCsEOja1Uu"
      }
    }
  ]
}
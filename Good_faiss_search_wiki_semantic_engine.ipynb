{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNl36AyoXIyzv2wy/r68l3X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahimku2020/fahimku2020/blob/main/Good_faiss_search_wiki_semantic_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJEbv7NcrEH7",
        "outputId": "89ffc196-4e49-4d25-fe97-62bbeb1a2728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install wikipedia\n",
        "!pip install nltk\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import wikipedia\n",
        "import nltk\n",
        "import faiss  # For efficient similarity search\n",
        "import hashlib\n",
        "import functools\n",
        "\n",
        "# Download necessary NLTK data (if not already present)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load a pre-trained Sentence Transformer model\n",
        "model = SentenceTransformer('all-mpnet-base-v2')  # or any other suitable model\n",
        "\n",
        "# Cache for Wikipedia fetches and embeddings\n",
        "@functools.lru_cache(maxsize=128)  # Adjust cache size as needed\n",
        "def get_wikipedia_sentences(query):\n",
        "    try:\n",
        "        page = wikipedia.page(query)\n",
        "        sentences = nltk.sent_tokenize(page.content)\n",
        "        return sentences\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        print(f\"Error: Wikipedia page not found for '{query}'\")\n",
        "        return None\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        print(f\"Error: Disambiguation error for '{query}'. Options: {e.options}\")\n",
        "        return None\n",
        "\n",
        "# Cache for sentence embeddings\n",
        "@functools.lru_cache(maxsize=1024) # Adjust as needed\n",
        "def get_embeddings(sentences):\n",
        "    # Convert the list of sentences to a tuple before hashing\n",
        "    sentences_tuple = tuple(sentences)\n",
        "    # Now use sentences_tuple as the key for caching\n",
        "    return model.encode(sentences_tuple)\n",
        "\n",
        "\n",
        "\n",
        "def search_wikipedia(query, top_k=10):\n",
        "    sentences = get_wikipedia_sentences(query)\n",
        "    if sentences is None:\n",
        "        return []\n",
        "\n",
        "    query_embedding = model.encode([query])[0]\n",
        "    # Pass the list of sentences directly to get_embeddings\n",
        "    sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "    # FAISS for efficient similarity search\n",
        "    dim = sentence_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)  # Use FlatIP for cosine similarity\n",
        "    index.add(sentence_embeddings)\n",
        "\n",
        "    D, I = index.search(query_embedding.reshape(1, -1), top_k)  # Search for top_k\n",
        "    results = [(sentences[i], score) for i, score in zip(I[0], D[0])]\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Example usage\n",
        "query = \"Amitabh Bachan films \"\n",
        "results = search_wikipedia(query)\n",
        "\n",
        "if results:\n",
        "    print(f\"Top sentences related to '{query}':\\n\")\n",
        "    for sentence, score in results:\n",
        "        print(f\"Score: {score:.4f}: {sentence}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Chunking Example (if needed for very long Wikipedia articles):\n",
        "def chunk_text(text, chunk_size=500): # Adjust chunk_size as needed.\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    for sentence in sentences:\n",
        "        current_chunk.append(sentence)\n",
        "        current_length += len(sentence)\n",
        "        if current_length >= chunk_size:\n",
        "            chunks.append(\" \".join(current_chunk)) # Join sentences into a chunk\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Example chunking usage:\n",
        "#  page = wikipedia.page(\"Artificial intelligence\")\n",
        "#  chunks = chunk_text(page.content)\n",
        "#  # Then process chunks individually"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOmbmyd2sp9o",
        "outputId": "7aee2238-019f-49a1-e3f7-d01558e32a07"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top sentences related to 'Amitabh Bachan films ':\n",
            "\n",
            "Score: 0.7120: Amitabh Bachchan (pronounced [əmɪˈt̪ɑːbʱ ˈbətːʃən] ; born Amitabh Srivastava; 11 October 1942) is an Indian actor who works in Hindi cinema.\n",
            "\n",
            "Score: 0.6943: (2009)\n",
            "Ra.One (2011)\n",
            "Kahaani (2012)\n",
            "Krrish 3 (2013)\n",
            "Mahabharat (2013)\n",
            "Kochadaiiyaan (Hindi Version) (2014)\n",
            "CBI documentary (2014) – sanctioned by Central Bureau of Investigation\n",
            "The Ghazi Attack (2017)\n",
            "Firangi (2017)\n",
            "\n",
            "\n",
            "=== Business investments ===\n",
            "Around 1994, Bachchan started Amitabh Bachchan Corporation Ltd (ABCL), an event management, production and distribution company.\n",
            "\n",
            "Score: 0.6535: Bachchan attempted to revive his acting career, and eventually had commercial success with Bade Miyan Chote Miyan (1998) and Major Saab (1998), and received positive reviews for Sooryavansham (1999), but other films such as Lal Baadshah (1999) and Kohram (1999) were box office failures.\n",
            "\n",
            "Score: 0.6350: This was the first movie in which Bachchan played a double role.\n",
            "\n",
            "Score: 0.6184: Bachchan's performance in the film received acclaim.\n",
            "\n",
            "Score: 0.6142: Many of Bachchan's films during this early period did not do well.\n",
            "\n",
            "Score: 0.6114: Bachchan starred as a young poet, Amit Malhotra, who falls deeply in love with a beautiful young girl named Pooja (Rakhee Gulzar) who ends up marrying someone else (Shashi Kapoor).\n",
            "\n",
            "Score: 0.6107: Since then he starred in several successful and acclaimed films like Kabhi Khushi Kabhie Gham, Aankhen, Baghban, Khakee, Black, Bunty Aur Babli, Sarkar, Kabhi Alvida Naa Kehna, Bhoothnath, Cheeni Kum, Paa, Piku, Pink, Badla, Brahmāstra: Part One – Shiva and Kalki 2898 AD.\n",
            "\n",
            "Score: 0.6054: Other hits followed, with Bachchan appearing as an older family patriarch in Ek Rishtaa: The Bond of Love (2001), Kabhi Khushi Kabhie Gham... (2001) and Baghban (2003).\n",
            "\n",
            "Score: 0.6037: In 2019, Bachchan appeared in Sujoy Ghosh's mystery thriller Badla.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
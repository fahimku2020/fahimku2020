{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbmINCeyhlI45qm8ZQjYRc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahimku2020/fahimku2020/blob/main/Semantic_serach_by_txtai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2S_yrzjoDsg",
        "outputId": "03ab56b8-5781-4cc0-99ee-077cb94c6220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: txtai in /usr/local/lib/python3.10/dist-packages (8.0.0)\n",
            "Requirement already satisfied: faiss-cpu>=1.7.1.post2 in /usr/local/lib/python3.10/dist-packages (from txtai) (1.9.0.post1)\n",
            "Requirement already satisfied: msgpack>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from txtai) (1.1.0)\n",
            "Requirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from txtai) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.45.0 in /usr/local/lib/python3.10/dist-packages (from txtai) (4.46.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from txtai) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.18.4 in /usr/local/lib/python3.10/dist-packages (from txtai) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from txtai) (6.0.2)\n",
            "Requirement already satisfied: regex>=2022.8.17 in /usr/local/lib/python3.10/dist-packages (from txtai) (2024.9.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu>=1.7.1.post2->txtai) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->txtai) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->txtai) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->txtai) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->txtai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->txtai) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->txtai) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->txtai) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->txtai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.12.1->txtai) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.45.0->txtai) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.45.0->txtai) (0.4.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.1->txtai) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->txtai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->txtai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->txtai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->txtai) (2024.8.30)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install txtai\n",
        "!pip install wikipedia\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from txtai.embeddings import Embeddings\n",
        "\n",
        "def fetch_wikipedia_content(query):\n",
        "    \"\"\"\n",
        "    Fetch Wikipedia content for a given query using Wikipedia API\n",
        "    \"\"\"\n",
        "    # Wikipedia API endpoint\n",
        "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    # Parameters for API request\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": query,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,  # Get only the introduction\n",
        "        \"explaintext\": True  # Get plain text\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Send request to Wikipedia API\n",
        "        response = requests.get(base_url, params=params)\n",
        "        data = response.json()\n",
        "\n",
        "        # Extract the page content\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        for page_id, page_info in pages.items():\n",
        "            if page_id != '-1':  # Valid page\n",
        "                return page_info.get('extract', 'No content found')\n",
        "\n",
        "        return \"No content found for the given query\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching content: {str(e)}\"\n",
        "\n",
        "def perform_semantic_search(document, query):\n",
        "    \"\"\"\n",
        "    Perform semantic search on the document using txtai\n",
        "    \"\"\"\n",
        "    # Split document into sentences\n",
        "    sentences = document.split('. ')\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = Embeddings()\n",
        "\n",
        "    # Index the sentences\n",
        "    embeddings.index(sentences)\n",
        "\n",
        "    # Perform semantic search\n",
        "    results = embeddings.search(query, 5)  # Top 5 most relevant sentences\n",
        "\n",
        "    return [sentences[result[0]] for result in results]\n",
        "\n",
        "def main():\n",
        "    # Get topic from user\n",
        "    topic = input(\"Enter a Wikipedia topic to search: \")\n",
        "\n",
        "    # Fetch Wikipedia content\n",
        "    document = fetch_wikipedia_content(topic)\n",
        "\n",
        "    # Get search query from user\n",
        "    search_query = input(\"Enter your semantic search query: \")\n",
        "\n",
        "    # Perform semantic search\n",
        "    relevant_sentences = perform_semantic_search(document, search_query)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nMost Relevant Sentences:\")\n",
        "    for i, sentence in enumerate(relevant_sentences, 1):\n",
        "        print(f\"{i}. {sentence}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IHP5UTvsBEF",
        "outputId": "ac8e564c-c6ce-48a2-8ec8-9ce0c6adb212"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a Wikipedia topic to search: Artificial intelligence \n",
            "Enter your semantic search query: Machine learning, supervise learning, computer vision\n",
            "\n",
            "Most Relevant Sentences:\n",
            "1. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals\n",
            "2. Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems\n",
            "3. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics\n",
            "4. Such machines may be called AIs.\n",
            "Some high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go)\n",
            "5. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from txtai.embeddings import Embeddings\n",
        "\n",
        "def fetch_wikipedia_content(query):\n",
        "    \"\"\"Fetch Wikipedia content for a given query.\"\"\"\n",
        "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": query,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "        \"explaintext\": True\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract the page content\n",
        "    pages = data.get('query', {}).get('pages', {})\n",
        "    for page_id, page_info in pages.items():\n",
        "        return page_info.get('extract', 'No content found')\n",
        "\n",
        "    return 'No content found'\n",
        "\n",
        "def perform_semantic_search(document, queries):\n",
        "    \"\"\"Perform semantic search on the document for multiple queries.\"\"\"\n",
        "    # Split document into sentences\n",
        "    sentences = document.split('. ')\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = Embeddings()\n",
        "    embeddings.index(sentences)\n",
        "\n",
        "    # Search results for each query\n",
        "    results = {}\n",
        "    for query in queries:\n",
        "        # Perform semantic search\n",
        "        search_results = embeddings.search(query, 3)\n",
        "\n",
        "        # Get relevant sentences\n",
        "        relevant_sentences = [sentences[result[0]] for result in search_results]\n",
        "        results[query] = relevant_sentences\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    # Get Wikipedia topic from user\n",
        "    topic = input(\"Enter a Wikipedia topic to search: \")\n",
        "\n",
        "    # Fetch Wikipedia content\n",
        "    document = fetch_wikipedia_content(topic)\n",
        "\n",
        "    # Get multiple search queries from user\n",
        "    queries = []\n",
        "    while True:\n",
        "        query = input(\"Enter a search query (or press Enter to finish): \")\n",
        "        if not query:\n",
        "            break\n",
        "        queries.append(query)\n",
        "\n",
        "    # Perform semantic search\n",
        "    search_results = perform_semantic_search(document, queries)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nSearch Results:\")\n",
        "    for query, sentences in search_results.items():\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        for i, sentence in enumerate(sentences, 1):\n",
        "            print(f\"{i}. {sentence}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMHA5n3Xv7ve",
        "outputId": "51285c46-c7cd-434f-c45e-4d51fba61058"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a Wikipedia topic to search: Artificial intelligence \n",
            "Enter a search query (or press Enter to finish): Computer  vision  applications \n",
            "Enter a search query (or press Enter to finish): Machine learning impact \n",
            "Enter a search query (or press Enter to finish): Nlp advantages\n",
            "Enter a search query (or press Enter to finish): \n",
            "\n",
            "Search Results:\n",
            "\n",
            "Query: Computer  vision  applications \n",
            "1. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals\n",
            "2. Such machines may be called AIs.\n",
            "Some high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go)\n",
            "3. However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\n",
            "The various subfields of AI research are centered around particular goals and the use of particular tools\n",
            "\n",
            "Query: Machine learning impact \n",
            "1. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals\n",
            "2. The widespread use of AI in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n",
            "\n",
            "\n",
            "3. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques\n",
            "\n",
            "Query: Nlp advantages\n",
            "1. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics\n",
            "2. The widespread use of AI in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n",
            "\n",
            "\n",
            "3. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MPGEiycx0wCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4U2_srH0v7in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7mL78KGisA2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Sf9qOMPN0v5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N3s5r5pEvJiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SQlLkwyksAtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-yaInn5Yp8MK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sPJNPiJhoES8"
      }
    }
  ]
}
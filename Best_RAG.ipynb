{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvKaIjfVNcdGM/bbvFDQCQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahimku2020/fahimku2020/blob/main/Best_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtOhu428X8MK",
        "outputId": "1df4c644-967d-41a2-dbba-0d23a592d34d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Define a function for retrieval\n",
        "def retrieve_documents(query):\n",
        "    # Use your retrieval method here, e.g., Elasticsearch or dense vector retrieval\n",
        "    # Return a list of relevant documents\n",
        "    # Sample relevant documents for the query \"Tell me about Albert Einstein\"\n",
        "    relevant_documents = [\n",
        "        \"Albert Einstein was a famous physicist who developed the theory of relativity.\",\n",
        "        \"He was born on March 14, 1879, in Ulm, Germany, and died on April 18, 1955, in Princeton, New Jersey, USA.\",\n",
        "        \"Einstein's most famous equation is E=mc^2, which relates energy (E) to mass (m) and the speed of light (c).\",\n",
        "        \"He won the Nobel Prize in Physics in 1921 for his work on the photoelectric effect.\",\n",
        "        \"Albert Einstein's contributions to science revolutionized our understanding of the universe.\"\n",
        "    ]\n",
        "    return relevant_documents\n",
        "\n",
        "# Define a function for generation\n",
        "def generate_response(relevant_documents):\n",
        "    # Use a pre-trained language model for text generation\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "    # Concatenate the relevant documents into a single string\n",
        "    context = \" \".join(relevant_documents)\n",
        "\n",
        "    # Generate text based on the retrieved information\n",
        "    generated_text = generator(context, max_length=1000)[0][\"generated_text\"]\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "query = \"Tell me about Albert Einstein nobel priize\"\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "response = generate_response(retrieved_docs)\n",
        "print(response)"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZ+QCU1zL0i0yM8GHyP5So",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahimku2020/fahimku2020/blob/main/Fast_keywords_extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBaIFYAex9-L",
        "outputId": "144d4bbf-6705-4872-d6e0-8584bf811c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=0fe2f2367fbd07afe6ee315de091e926b7f210e07793ce578beeb0a4d14e6884\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting lru_cache\n",
            "  Downloading lru_cache-0.2.3.tar.gz (2.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: lru_cache\n",
            "  Building wheel for lru_cache (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lru_cache: filename=lru_cache-0.2.3-py3-none-any.whl size=3253 sha256=6f56b8725288747526e1594d3f07ed4f89dbb8b665563aca5ca4a7521524db7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/2a/5b/c05a1aaeef93ea12dca9e5e9f4b7b06479de6aaf63ddcde3b7\n",
            "Successfully built lru_cache\n",
            "Installing collected packages: lru_cache\n",
            "Successfully installed lru_cache-0.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia\n",
        "!pip install sentence_transformers\n",
        "!pip install nltk\n",
        "!pip install faiss-gpu\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install lru_cache\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "from functools import lru_cache\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import faiss\n",
        "import torch\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "nltk.download ('punkt_tab')\n",
        "\n",
        "class AdvancedKeywordExtractor:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2', num_clusters=5, cache_size=128):\n",
        "        \"\"\"\n",
        "        Initialize with specified transformer model and clustering parameters\n",
        "        \"\"\"\n",
        "        # Initialize NLTK resources\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "        # Initialize sentence transformer model\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = SentenceTransformer(model_name, device=self.device)\n",
        "\n",
        "        # Clustering parameters\n",
        "        self.num_clusters = num_clusters\n",
        "        self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "        # Initialize FAISS index\n",
        "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
        "        self.index = faiss.IndexFlatIP(self.dimension)\n",
        "\n",
        "        # Cache for embeddings and similarities\n",
        "        self.cache_size = cache_size\n",
        "\n",
        "    @lru_cache(maxsize=128)\n",
        "    def fetch_wikipedia_content(self, topic: str) -> str:\n",
        "        \"\"\"\n",
        "        Fetch and cache Wikipedia content\n",
        "        \"\"\"\n",
        "        try:\n",
        "            page = wikipedia.page(topic)\n",
        "            return page.content\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching Wikipedia content: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean and preprocess text\n",
        "        \"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove special characters and numbers\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Remove stopwords\n",
        "        words = word_tokenize(text)\n",
        "        words = [word for word in words if word not in self.stop_words]\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    @lru_cache(maxsize=128)\n",
        "    def get_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get and cache sentence embeddings\n",
        "        \"\"\"\n",
        "        return self.model.encode(text, convert_to_tensor=True).cpu().numpy()\n",
        "\n",
        "    def generate_ngrams(self, text: str, n_range: Tuple[int, int] = (2, 2)) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate n-grams efficiently\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(text)\n",
        "        all_ngrams = []\n",
        "\n",
        "        for n in range(n_range[0], n_range[1] + 1):\n",
        "            n_grams = list(ngrams(tokens, n))\n",
        "            all_ngrams.extend([' '.join(gram) for gram in n_grams])\n",
        "\n",
        "        return all_ngrams\n",
        "\n",
        "    def cluster_sentences(self, sentences: List[str]) -> Dict[int, List[str]]:\n",
        "        \"\"\"\n",
        "        Cluster sentences using FAISS for efficient similarity search\n",
        "        \"\"\"\n",
        "        # Get embeddings for all sentences\n",
        "        embeddings = np.vstack([self.get_embedding(sent) for sent in sentences])\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        faiss.normalize_L2(embeddings)\n",
        "\n",
        "        # Perform clustering\n",
        "        kmeans = faiss.Kmeans(embeddings.shape[1], self.num_clusters, niter=20, gpu=torch.cuda.is_available())\n",
        "        kmeans.train(embeddings)\n",
        "        _, labels = kmeans.index.search(embeddings, 1)\n",
        "\n",
        "        # Group sentences by cluster\n",
        "        clusters = defaultdict(list)\n",
        "        for sent, label in zip(sentences, labels):\n",
        "            clusters[int(label[0])].append(sent)\n",
        "\n",
        "        return dict(clusters)\n",
        "\n",
        "    def calculate_similarity_matrix(self, sentences: List[str], keywords: List[str]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate similarity matrix using FAISS\n",
        "        \"\"\"\n",
        "        sent_embeddings = np.vstack([self.get_embedding(sent) for sent in sentences])\n",
        "        keyword_embeddings = np.vstack([self.get_embedding(keyword) for keyword in keywords])\n",
        "\n",
        "        # Normalize embeddings\n",
        "        faiss.normalize_L2(sent_embeddings)\n",
        "        faiss.normalize_L2(keyword_embeddings)\n",
        "\n",
        "        # Calculate similarities using FAISS\n",
        "        similarities = keyword_embeddings @ sent_embeddings.T\n",
        "\n",
        "        return similarities\n",
        "\n",
        "    def max_sum_similarity(self, similarities: np.ndarray, top_n: int = 5) -> List[int]:\n",
        "        \"\"\"\n",
        "        Select top sentences using max sum similarity\n",
        "        \"\"\"\n",
        "        row_sums = similarities.sum(axis=0)\n",
        "        return row_sums.argsort()[-top_n:][::-1]\n",
        "\n",
        "    def extract_keywords_and_sentences(self, topic: str,\n",
        "                                    max_keywords: int = 10,\n",
        "                                    sentences_per_cluster: int = 3) -> Dict:\n",
        "        \"\"\"\n",
        "        Main function to extract keywords and relevant sentences\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Fetch and preprocess content\n",
        "        content = self.fetch_wikipedia_content(topic)\n",
        "        if not content:\n",
        "            return {}\n",
        "\n",
        "        # Split into sentences and clean\n",
        "        sentences = sent_tokenize(content)\n",
        "        cleaned_sentences = [self.clean_text(sent) for sent in sentences]\n",
        "\n",
        "        # Cluster sentences\n",
        "        clusters = self.cluster_sentences(cleaned_sentences)\n",
        "\n",
        "        # Process each cluster in parallel\n",
        "        results = defaultdict(dict)\n",
        "\n",
        "        def process_cluster(cluster_id, cluster_sentences):\n",
        "            # Generate n-grams for the cluster\n",
        "            cluster_text = ' '.join(cluster_sentences)\n",
        "            ngrams_list = self.generate_ngrams(cluster_text)\n",
        "\n",
        "            # Get embeddings and calculate similarities\n",
        "            similarities = self.calculate_similarity_matrix(cluster_sentences, ngrams_list)\n",
        "\n",
        "            # Select top keywords and sentences\n",
        "            top_indices = self.max_sum_similarity(similarities, top_n=max_keywords)\n",
        "            top_keywords = [ngrams_list[i] for i in top_indices]\n",
        "\n",
        "            # Get most relevant sentences\n",
        "            sent_similarities = self.calculate_similarity_matrix(cluster_sentences, top_keywords)\n",
        "            top_sent_indices = self.max_sum_similarity(sent_similarities, top_n=sentences_per_cluster)\n",
        "            top_sentences = [sentences[i] for i in top_sent_indices]\n",
        "\n",
        "            return {\n",
        "                'keywords': top_keywords,\n",
        "                'sentences': top_sentences,\n",
        "                'size': len(cluster_sentences)\n",
        "            }\n",
        "\n",
        "        # Process clusters in parallel\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            future_to_cluster = {\n",
        "                executor.submit(process_cluster, cluster_id, cluster_sentences): cluster_id\n",
        "                for cluster_id, cluster_sentences in clusters.items()\n",
        "            }\n",
        "\n",
        "            for future in tqdm(future_to_cluster, desc=\"Processing clusters\"):\n",
        "                cluster_id = future_to_cluster[future]\n",
        "                results[cluster_id] = future.result()\n",
        "\n",
        "        # Add execution time\n",
        "        execution_time = time.time() - start_time\n",
        "        results['metadata'] = {\n",
        "            'execution_time': execution_time,\n",
        "            'num_clusters': len(clusters),\n",
        "            'total_sentences': len(sentences)\n",
        "        }\n",
        "\n",
        "        return dict(results)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize extractor\n",
        "    extractor = AdvancedKeywordExtractor()\n",
        "\n",
        "    # Extract keywords and sentences\n",
        "    topic = \"Amitabh  Bachan \"\n",
        "    results = extractor.extract_keywords_and_sentences(\n",
        "        topic,\n",
        "        max_keywords=10,\n",
        "        sentences_per_cluster=5\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nResults for topic: {topic}\")\n",
        "    print(f\"Execution time: {results['metadata']['execution_time']:.2f} seconds\")\n",
        "    print(f\"Number of clusters: {results['metadata']['num_clusters']}\")\n",
        "\n",
        "    for cluster_id, cluster_data in results.items():\n",
        "        if cluster_id != 'metadata':\n",
        "            print(f\"\\nCluster {cluster_id} (Size: {cluster_data['size']})\")\n",
        "            print(\"\\nTop Keywords:\")\n",
        "            for keyword in cluster_data['keywords'][:5]:\n",
        "                print(f\"- {keyword}\")\n",
        "            print(\"\\nRelevant Sentences:\")\n",
        "            for sentence in cluster_data['sentences']:\n",
        "                print(f\"- {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koNcs3Rlx-6-",
        "outputId": "d411ef56-2bab-45b8-a26b-ea42b4399868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Processing clusters: 100%|██████████| 5/5 [00:45<00:00,  9.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for topic: Amitabh  Bachan \n",
            "Execution time: 48.03 seconds\n",
            "Number of clusters: 5\n",
            "\n",
            "Cluster 1 (Size: 58)\n",
            "\n",
            "Top Keywords:\n",
            "- hailed shahenshah\n",
            "- awards sixteen\n",
            "- shahenshah bollywood\n",
            "- six national\n",
            "- bachchan often\n",
            "\n",
            "Relevant Sentences:\n",
            "- With a cinematic career spanning over five decades, he has played pivotal roles in over 200 films.\n",
            "- During this time, he made a guest appearance in the film Guddi which starred his future wife Jaya Bhaduri.\n",
            "- His first acting role was as one of the seven protagonists in the film Saat Hindustani, directed by Khwaja Ahmad Abbas and featuring Utpal Dutt, Anwar Ali (brother of comedian Mehmood), Madhu and Jalal Agha.\n",
            "- Amitabh Bachchan (pronounced [əmɪˈt̪ɑːbʱ ˈbətːʃən] ; born Amitabh Srivastava; 11 October 1942) is an Indian actor who works in Hindi cinema.\n",
            "- Harivansh's ancestors came from a village called Babupatti, in the Raniganj tehsil, in the Pratapgarh district, in the present-day state of Uttar Pradesh, in India.\n",
            "\n",
            "Cluster 2 (Size: 81)\n",
            "\n",
            "Top Keywords:\n",
            "- played pivotal\n",
            "- years dubbed\n",
            "- later years\n",
            "- greater stardom\n",
            "- earlys films\n",
            "\n",
            "Relevant Sentences:\n",
            "- He is a recipient of several accolades including six National Film Awards and sixteen Filmfare Awards.\n",
            "- Bachchan's parents were initially going to name him Inquilaab (Hindustani for \"Revolution\"), inspired by the phrase Inquilab Zindabad (\"Long live the revolution\") popularly used during the Indian independence struggle; the name Amitabh was suggested to his father by poet Sumitranandan Pant.\n",
            "- Bachchan is often hailed as the Shahenshah of Bollywood, Sadi Ke Mahanayak (translated as \"Greatest actor of the century\" in Hindi), Star of the Millennium, or simply Big B.\n",
            "- Amitabh Bachchan (pronounced [əmɪˈt̪ɑːbʱ ˈbətːʃən] ; born Amitabh Srivastava; 11 October 1942) is an Indian actor who works in Hindi cinema.\n",
            "- Bachchan's father died in 2003, and his mother in 2007.\n",
            "\n",
            "Cluster 3 (Size: 96)\n",
            "\n",
            "Top Keywords:\n",
            "- poll october\n",
            "- prayagraj hindi\n",
            "- involved several\n",
            "- poet harivansh\n",
            "- bachchan voted\n",
            "\n",
            "Relevant Sentences:\n",
            "- She spent 48 days at Bachchan's house with his parents before her wedding to Rajiv.\n",
            "- Amitabh Bachchan was born in 1942 in Allahabad (now Prayagraj) to the Hindi poet Harivansh Rai Bachchan and his wife, the social activist Teji Bachchan.\n",
            "- Amitabh Bachchan (pronounced [əmɪˈt̪ɑːbʱ ˈbətːʃən] ; born Amitabh Srivastava; 11 October 1942) is an Indian actor who works in Hindi cinema.\n",
            "- He was educated at Sherwood College, Nainital, and Kirori Mal College, University of Delhi.\n",
            "- He is a recipient of several accolades including six National Film Awards and sixteen Filmfare Awards.\n",
            "\n",
            "Cluster 4 (Size: 52)\n",
            "\n",
            "Top Keywords:\n",
            "- including africa\n",
            "- banega crorepati\n",
            "- diaspora well\n",
            "- indian subcontinent\n",
            "- nainital kirori\n",
            "\n",
            "Relevant Sentences:\n",
            "- With a cinematic career spanning over five decades, he has played pivotal roles in over 200 films.\n",
            "- Bachchan has won numerous accolades in his career, including record four National Film Awards in Best Actor category and many awards at international film festivals and award ceremonies.\n",
            "- He attended Kirori Mal College at the University of Delhi in Delhi.\n",
            "- She spent 48 days at Bachchan's house with his parents before her wedding to Rajiv.\n",
            "- He was educated at Sherwood College, Nainital, and Kirori Mal College, University of Delhi.\n",
            "\n",
            "Cluster 0 (Size: 48)\n",
            "\n",
            "Top Keywords:\n",
            "- insisted take\n",
            "- choice career\n",
            "- also entered\n",
            "- exceptional career\n",
            "- always insisted\n",
            "\n",
            "Relevant Sentences:\n",
            "- Bachchan's parents were initially going to name him Inquilaab (Hindustani for \"Revolution\"), inspired by the phrase Inquilab Zindabad (\"Long live the revolution\") popularly used during the Indian independence struggle; the name Amitabh was suggested to his father by poet Sumitranandan Pant.\n",
            "- With a cinematic career spanning over five decades, he has played pivotal roles in over 200 films.\n",
            "- He is a recipient of several accolades including six National Film Awards and sixteen Filmfare Awards.\n",
            "- He has hosted several seasons of the game show Kaun Banega Crorepati, India's version of the game show franchise, Who Wants to Be a Millionaire?.\n",
            "- Bachchan has won numerous accolades in his career, including record four National Film Awards in Best Actor category and many awards at international film festivals and award ceremonies.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3JiAFRTd4Ori"
      }
    }
  ]
}
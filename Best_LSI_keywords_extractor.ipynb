{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPECfsojHcQgWcPxtI0C3ac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahimku2020/fahimku2020/blob/main/Best_LSI_keywords_extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTQFwVevjeZb",
        "outputId": "cf745519-f8e4-4991-8936-5b505d9c2d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn\n",
        "!pip install pandas\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "class LSIKeywordExtractor:\n",
        "    def __init__(self, max_features=1000, n_components=100, top_k_keywords=10):\n",
        "        \"\"\"\n",
        "        Initialize LSI Keyword Extractor\n",
        "\n",
        "        Parameters:\n",
        "        - max_features: Maximum number of features (words) to consider\n",
        "        - n_components: Number of semantic dimensions to reduce to\n",
        "        - top_k_keywords: Number of top keywords to extract\n",
        "        \"\"\"\n",
        "        self.max_features = max_features\n",
        "        self.n_components = n_components\n",
        "        self.top_k_keywords = top_k_keywords\n",
        "\n",
        "        # TF-IDF Vectorizer\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            max_features=self.max_features,\n",
        "            stop_words='english',\n",
        "            use_idf=True\n",
        "        )\n",
        "\n",
        "        # LSI Components\n",
        "        self.lsi = None\n",
        "        self.feature_names = None\n",
        "\n",
        "    def fit(self, documents):\n",
        "        \"\"\"\n",
        "        Fit the LSI model to the documents\n",
        "\n",
        "        Parameters:\n",
        "        - documents: List of text documents\n",
        "        \"\"\"\n",
        "        # Create TF-IDF matrix\n",
        "        tfidf_matrix = self.vectorizer.fit_transform(documents)\n",
        "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
        "\n",
        "        # Perform Truncated SVD (LSI)\n",
        "        self.lsi = TruncatedSVD(n_components=self.n_components, random_state=42)\n",
        "        lsi_matrix = self.lsi.fit_transform(tfidf_matrix)\n",
        "\n",
        "        # Normalize the LSI matrix\n",
        "        self.lsi_normalized = Normalizer(copy=False).fit_transform(lsi_matrix)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def extract_keywords(self, document):\n",
        "        \"\"\"\n",
        "        Extract top keywords for a given document\n",
        "\n",
        "        Parameters:\n",
        "        - document: Text document to extract keywords from\n",
        "\n",
        "        Returns:\n",
        "        - List of top keywords with their weights\n",
        "        \"\"\"\n",
        "        if self.lsi is None:\n",
        "            raise ValueError(\"Model must be fit before extracting keywords\")\n",
        "\n",
        "        # Transform the document using TF-IDF\n",
        "        doc_tfidf = self.vectorizer.transform([document])\n",
        "\n",
        "        # Transform to LSI space\n",
        "        doc_lsi = self.lsi.transform(doc_tfidf)\n",
        "        doc_lsi_normalized = Normalizer().fit_transform(doc_lsi)\n",
        "\n",
        "        # Calculate keyword importance\n",
        "        keyword_importances = np.abs(self.lsi.components_.T @ doc_lsi_normalized.T).flatten()\n",
        "\n",
        "        # Get top keywords\n",
        "        top_indices = keyword_importances.argsort()[-self.top_k_keywords:][::-1]\n",
        "\n",
        "        # Create results with keywords and their importances\n",
        "        keywords = [\n",
        "            (self.feature_names[idx], keyword_importances[idx])\n",
        "            for idx in top_indices\n",
        "        ]\n",
        "\n",
        "        return keywords\n",
        "\n",
        "    def extract_corpus_keywords(self, documents):\n",
        "        \"\"\"\n",
        "        Extract top keywords for an entire corpus\n",
        "\n",
        "        Parameters:\n",
        "        - documents: List of text documents\n",
        "\n",
        "        Returns:\n",
        "        - DataFrame with top keywords across the corpus\n",
        "        \"\"\"\n",
        "        # Fit the model first\n",
        "        self.fit(documents)\n",
        "\n",
        "        # Extract keywords for each document\n",
        "        corpus_keywords = [\n",
        "            self.extract_keywords(doc) for doc in documents\n",
        "        ]\n",
        "\n",
        "        # Create a DataFrame for better visualization\n",
        "        df_keywords = pd.DataFrame({\n",
        "            'document': range(len(documents)),\n",
        "            'keywords': corpus_keywords\n",
        "        })\n",
        "\n",
        "        return df_keywords\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Sample documents\n",
        "    documents = [\n",
        "        \"Machine learning is a method of data analysis that automates analytical model building.\",\n",
        "        \"Python is a popular programming language for data science and artificial intelligence.\",\n",
        "        \"Natural language processing helps computers understand and interpret human language.\",\n",
        "        \"Deep learning is a subset of machine learning based on artificial neural networks.\"\n",
        "    ]\n",
        "\n",
        "    # Initialize and fit the LSI Keyword Extractor\n",
        "    extractor = LSIKeywordExtractor(\n",
        "        max_features=1000,\n",
        "        n_components=3,\n",
        "        top_k_keywords=5\n",
        "    )\n",
        "\n",
        "    # Extract keywords for the corpus\n",
        "    corpus_keywords = extractor.extract_corpus_keywords(documents)\n",
        "    print(\"Corpus Keywords:\")\n",
        "    print(corpus_keywords)\n",
        "\n",
        "    # Extract keywords for a specific document\n",
        "    test_document = \"Deep learning algorithms are revolutionizing artificial intelligence research.\"\n",
        "    keywords = extractor.extract_keywords(test_document)\n",
        "    print(\"\\nKeywords for test document:\")\n",
        "    for keyword, weight in keywords:\n",
        "        print(f\"{keyword}: {weight:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMHoLwBZjfGm",
        "outputId": "e738f1e8-224a-45bc-dc9a-08420c403ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus Keywords:\n",
            "   document                                           keywords\n",
            "0         0  [(learning, 0.5196712021288857), (machine, 0.3...\n",
            "1         1  [(science, 0.38139984838473195), (python, 0.38...\n",
            "2         2  [(language, 0.5120242788456416), (natural, 0.3...\n",
            "3         3  [(learning, 0.5235880361379652), (machine, 0.3...\n",
            "\n",
            "Keywords for test document:\n",
            "learning: 0.3942\n",
            "data: 0.3137\n",
            "artificial: 0.3008\n",
            "machine: 0.2671\n",
            "science: 0.2203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "class WikipediaLSIKeywordExtractor:\n",
        "    def __init__(self):\n",
        "        # Download necessary NLTK resources\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "\n",
        "        # Initialize lemmatizer and stopwords\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess the text by:\n",
        "        1. Lowercasing\n",
        "        2. Removing special characters\n",
        "        3. Tokenization\n",
        "        4. Removing stopwords\n",
        "        5. Lemmatization\n",
        "        \"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove special characters and digits\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords and lemmatize\n",
        "        processed_tokens = [\n",
        "            self.lemmatizer.lemmatize(token)\n",
        "            for token in tokens\n",
        "            if token not in self.stop_words and len(token) > 2\n",
        "        ]\n",
        "\n",
        "        return processed_tokens\n",
        "\n",
        "    def generate_bigrams(self, tokens):\n",
        "        \"\"\"\n",
        "        Generate bigrams from tokens\n",
        "        \"\"\"\n",
        "        return list(ngrams(tokens, 2))\n",
        "\n",
        "    def extract_keywords_lsi(self, text, num_keywords=20, num_topics=10):\n",
        "        \"\"\"\n",
        "        Extract keywords using LSI technique\n",
        "        \"\"\"\n",
        "        # Preprocess text\n",
        "        processed_tokens = self.preprocess_text(text)\n",
        "\n",
        "        # Generate unigrams and bigrams\n",
        "        unigrams = processed_tokens\n",
        "        bigrams = self.generate_bigrams(processed_tokens)\n",
        "\n",
        "        # Convert bigrams to strings\n",
        "        bigram_strings = [' '.join(bg) for bg in bigrams]\n",
        "\n",
        "        # Combine unigrams and bigrams\n",
        "        all_tokens = unigrams + bigram_strings\n",
        "\n",
        "        # Create TF-IDF vectorizer\n",
        "        vectorizer = TfidfVectorizer(token_pattern=r'\\b\\w+\\b|\\b\\w+\\s\\w+\\b')\n",
        "        tfidf_matrix = vectorizer.fit_transform(all_tokens)\n",
        "\n",
        "        # Perform LSI (Truncated SVD)\n",
        "        lsi = TruncatedSVD(n_components=num_topics, random_state=42)\n",
        "        lsi_matrix = lsi.fit_transform(tfidf_matrix)\n",
        "\n",
        "        # Get feature names\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        # Calculate keyword scores based on LSI components\n",
        "        keyword_scores = np.abs(lsi.components_).sum(axis=0)\n",
        "\n",
        "        # Get top keywords\n",
        "        top_keyword_indices = keyword_scores.argsort()[-num_keywords:][::-1]\n",
        "        top_keywords = [feature_names[i] for i in top_keyword_indices]\n",
        "\n",
        "        return top_keywords\n",
        "\n",
        "def fetch_wikipedia_content(topic):\n",
        "    \"\"\"\n",
        "    Manually fetch Wikipedia content by constructing a URL\n",
        "    Note: This is a simplified approach and might not work for all topics\n",
        "    \"\"\"\n",
        "    base_url = \"https://en.wikipedia.org/w/index.php\"\n",
        "    params = {\n",
        "        \"title\": topic.replace(\" \", \"_\"),\n",
        "        \"action\": \"render\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Simple text extraction (very basic)\n",
        "        text = re.sub(r'<.*?>', '', response.text)\n",
        "        text = re.sub(r'\\n+', ' ', text)\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching Wikipedia content: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def main():\n",
        "    # User input for Wikipedia topic\n",
        "    topic = input(\"Enter a Wikipedia topic to extract keywords: \")\n",
        "\n",
        "    # Fetch Wikipedia content\n",
        "    wiki_text = fetch_wikipedia_content(topic)\n",
        "\n",
        "    if not wiki_text:\n",
        "        print(\"Could not fetch content. Please try another topic.\")\n",
        "        return\n",
        "\n",
        "    # Initialize keyword extractor\n",
        "    extractor = WikipediaLSIKeywordExtractor()\n",
        "\n",
        "    # Extract keywords\n",
        "    keywords = extractor.extract_keywords_lsi(wiki_text)\n",
        "\n",
        "    print(f\"\\nTop Keywords for '{topic}':\")\n",
        "    for keyword in keywords:\n",
        "        print(keyword)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEEOI7NYm8vS",
        "outputId": "d4ae2f6c-75a6-41ee-e8b3-9541f0b246f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a Wikipedia topic to extract keywords: Amitabh Bachan \n",
            "\n",
            "Top Keywords for 'Amitabh Bachan ':\n",
            "archived\n",
            "original\n",
            "october\n",
            "july\n",
            "amitabh\n",
            "bachchan\n",
            "retrieved\n",
            "india\n",
            "film\n",
            "khan\n",
            "time\n",
            "december\n",
            "november\n",
            "rukh\n",
            "september\n",
            "office\n",
            "february\n",
            "march\n",
            "shah\n",
            "award\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "def fetch_wikipedia_content(topic):\n",
        "    \"\"\"\n",
        "    Fetch Wikipedia content by scraping the first section of the article.\n",
        "\n",
        "    Args:\n",
        "        topic (str): Wikipedia topic to search\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text content\n",
        "    \"\"\"\n",
        "    # Construct Wikipedia URL\n",
        "    url = f\"https://en.wikipedia.org/wiki/{topic.replace(' ', '_')}\"\n",
        "\n",
        "    try:\n",
        "        # Send request to Wikipedia\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract text from the first paragraphs\n",
        "        paragraphs = soup.select('#mw-content-text p')\n",
        "\n",
        "        # Combine paragraphs, limit to first 5 for processing\n",
        "        content = ' '.join([p.get_text() for p in paragraphs[:5]])\n",
        "\n",
        "        return content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching Wikipedia content: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess text by removing special characters and converting to lowercase.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Preprocessed text\n",
        "    \"\"\"\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "    return text\n",
        "\n",
        "def extract_lsi_bigram_keywords(text, num_keywords=10):\n",
        "    \"\"\"\n",
        "    Extract LSI keywords using bigrams.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text\n",
        "        num_keywords (int): Number of keywords to extract\n",
        "\n",
        "    Returns:\n",
        "        list: Extracted bigram keywords\n",
        "    \"\"\"\n",
        "    # Preprocess text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # Generate bigrams\n",
        "    tokens = nltk.word_tokenize(preprocessed_text)\n",
        "    bigram_tokens = list(ngrams(tokens, 2))\n",
        "\n",
        "    # Convert bigrams to strings\n",
        "    bigram_strings = [' '.join(bg) for bg in bigram_tokens]\n",
        "\n",
        "    # Create TF-IDF Vectorizer\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
        "    tfidf_matrix = vectorizer.fit_transform(bigram_strings)\n",
        "\n",
        "    # Perform LSI (Truncated SVD)\n",
        "    lsi = TruncatedSVD(n_components=1, random_state=42)\n",
        "    lsi_matrix = lsi.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # Get feature names (bigrams)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Calculate importance scores\n",
        "    importance_scores = np.abs(lsi.components_[0])\n",
        "\n",
        "    # Sort bigrams by importance\n",
        "    sorted_indices = importance_scores.argsort()[::-1]\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    unique_keywords = []\n",
        "    for idx in sorted_indices:\n",
        "        keyword = feature_names[idx]\n",
        "        if keyword not in unique_keywords:\n",
        "            unique_keywords.append(keyword)\n",
        "\n",
        "    # Return top N unique keywords\n",
        "    return unique_keywords[:num_keywords]\n",
        "\n",
        "def main():\n",
        "    # Get user input for Wikipedia topic\n",
        "    topic = input(\"Enter a Wikipedia topic to extract keywords: \")\n",
        "\n",
        "    # Fetch Wikipedia content\n",
        "    wiki_content = fetch_wikipedia_content(topic)\n",
        "\n",
        "    if wiki_content:\n",
        "        # Extract LSI bigram keywords\n",
        "        keywords = extract_lsi_bigram_keywords(wiki_content)\n",
        "\n",
        "        print(f\"\\nTop Bigram Keywords for '{topic}':\")\n",
        "        for i, keyword in enumerate(keywords, 1):\n",
        "            print(f\"{i}. {keyword}\")\n",
        "    else:\n",
        "        print(\"Could not retrieve content for the given topic.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yqDG4H2oW45",
        "outputId": "581b8242-ea1e-4e6a-e417-1a59f37b9c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a Wikipedia topic to extract keywords: Amitabh Bachan \n",
            "\n",
            "Top Bigram Keywords for 'Amitabh Bachan ':\n",
            "1. in the\n",
            "2. national film\n",
            "3. best actor\n",
            "4. star of\n",
            "5. he is\n",
            "6. bachchan has\n",
            "7. he has\n",
            "8. of cinema\n",
            "9. roles in\n",
            "10. of his\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNVRS_p-r2rm",
        "outputId": "461824cd-f4dc-430b-cb23-7d0d5ba0e8b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=6c3b2e44e724b588632f70a1e5b87a8b35e62d8fa7a5970ab0b91db7d44260cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8tVBTfbsr2bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import wikipedia\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess text by:\n",
        "    1. Splitting into sentences\n",
        "    2. Removing stopwords\n",
        "    3. Converting to lowercase\n",
        "    \"\"\"\n",
        "    # Get English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Tokenize sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Preprocess each sentence\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Tokenize words\n",
        "        words = word_tokenize(sentence.lower())\n",
        "\n",
        "        # Remove stopwords and non-alphabetic tokens\n",
        "        filtered_words = [\n",
        "            word for word in words\n",
        "            if word.isalpha() and word not in stop_words\n",
        "        ]\n",
        "\n",
        "        processed_sentences.append(' '.join(filtered_words))\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "def extract_ngrams(words, n):\n",
        "    \"\"\"\n",
        "    Extract n-grams from a list of words\n",
        "    \"\"\"\n",
        "    return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "def lsi_keyword_extraction(text, n_keywords=20, n_grams=2):\n",
        "    \"\"\"\n",
        "    Extract keywords using Latent Semantic Indexing\n",
        "\n",
        "    Parameters:\n",
        "    - text: Input text\n",
        "    - n_keywords: Number of keywords to extract\n",
        "    - n_grams: Length of n-grams to consider\n",
        "    \"\"\"\n",
        "    # Preprocess text\n",
        "    processed_sentences = preprocess_text(text)\n",
        "\n",
        "    # Create TF-IDF Vectorizer\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
        "\n",
        "    # Perform LSI (SVD)\n",
        "    lsi = TruncatedSVD(n_components=min(n_keywords, len(processed_sentences)-1))\n",
        "    lsi_matrix = lsi.fit_transform(tfidf_matrix)\n",
        "\n",
        "    # Normalize the LSI matrix\n",
        "    lsi_matrix_normalized = Normalizer(copy=False).fit_transform(lsi_matrix)\n",
        "\n",
        "    # Get feature names (words)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Extract top keywords based on LSI components\n",
        "    keywords = []\n",
        "    for component in lsi.components_:\n",
        "        # Get top indices for this component\n",
        "        top_indices = component.argsort()[::-1][:n_keywords]\n",
        "        top_keywords = [feature_names[idx] for idx in top_indices]\n",
        "        keywords.extend(top_keywords)\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    keywords = list(dict.fromkeys(keywords))\n",
        "\n",
        "    # Generate n-grams\n",
        "    all_words = []\n",
        "    for sentence in processed_sentences:\n",
        "        all_words.extend(sentence.split())\n",
        "\n",
        "    ngrams = extract_ngrams(all_words, n_grams)\n",
        "\n",
        "    return {\n",
        "        'keywords': keywords[:n_keywords],\n",
        "        'ngrams': ngrams[:n_keywords]\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    # Get Wikipedia article\n",
        "    try:\n",
        "        # Prompt user for Wikipedia topic\n",
        "        topic = input(\"Enter a Wikipedia topic to extract keywords from: \")\n",
        "\n",
        "        # Fetch Wikipedia page\n",
        "        page = wikipedia.page(topic)\n",
        "\n",
        "        # Extract keywords\n",
        "        result = lsi_keyword_extraction(page.content)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n--- Keywords ---\")\n",
        "        for keyword in result['keywords']:\n",
        "            print(keyword)\n",
        "\n",
        "        print(\"\\n--- N-grams ---\")\n",
        "        for ngram in result['ngrams']:\n",
        "            print(ngram)\n",
        "\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        print(\"Multiple pages found. Please be more specific.\")\n",
        "        print(\"Possible options:\", e.options[:5])\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        print(\"No Wikipedia page found for the given topic.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUeOWMH1rxwv",
        "outputId": "69db46f3-79e1-4a57-bbe4-8a0e13bed44c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a Wikipedia topic to extract keywords from: Amitabh Bachan \n",
            "\n",
            "--- Keywords ---\n",
            "best\n",
            "award\n",
            "actor\n",
            "filmfare\n",
            "film\n",
            "bachchan\n",
            "performance\n",
            "role\n",
            "national\n",
            "supporting\n",
            "critics\n",
            "second\n",
            "year\n",
            "awards\n",
            "nominated\n",
            "fourth\n",
            "box\n",
            "office\n",
            "films\n",
            "amitabh\n",
            "\n",
            "--- N-grams ---\n",
            "amitabh bachchan\n",
            "bachchan pronounced\n",
            "pronounced ˈbətːʃən\n",
            "ˈbətːʃən born\n",
            "born amitabh\n",
            "amitabh srivastava\n",
            "srivastava october\n",
            "october indian\n",
            "indian actor\n",
            "actor works\n",
            "works hindi\n",
            "hindi cinema\n",
            "cinema often\n",
            "often considered\n",
            "considered one\n",
            "one greatest\n",
            "greatest accomplished\n",
            "accomplished commercially\n",
            "commercially successful\n",
            "successful actors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim import corpora, models, similarities\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import string\n",
        "# Download necessary NLTK data (only needs to be done once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extract_lsi_keywords(text, num_topics=5, num_keywords=5):\n",
        "    \"\"\"\n",
        "    Extracts keywords from text using Latent Semantic Indexing (LSI).\n",
        "\n",
        "    Args:\n",
        "        text: The input text.\n",
        "        num_topics: The number of topics to extract.\n",
        "        num_keywords: The number of keywords per topic.\n",
        "\n",
        "    Returns:\n",
        "        A list of keywords.\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    additional_stopwords = [\"may\",\"used\",\"use\",\"us\",\"said\", \"says\", \"would\", \"could\", \"should\", \"get\", \"go\", \"one\", \"two\", \"three\", \"many\", \"much\", \"also\", \"well\", \"even\", \"however\", \"therefore\", \"since\", \"although\", \"because\", \"though\"]\n",
        "    stop_words = set(stopwords.words('english')) | set(additional_stopwords)\n",
        "    punctuation = set(string.punctuation)\n",
        "\n",
        "\n",
        "    # Preprocessing: Tokenization, lowercasing, stop word removal, punctuation removal\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        words = [word for word in words if word.isalnum() and word not in stop_words ]\n",
        "        processed_sentences.append(words)\n",
        "\n",
        "    # Create a dictionary and corpus\n",
        "    dictionary = corpora.Dictionary(processed_sentences)\n",
        "    corpus = [dictionary.doc2bow(sentence) for sentence in processed_sentences]\n",
        "\n",
        "    # Train LSI model\n",
        "    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "    # Extract keywords (top words from each topic)\n",
        "\n",
        "    keywords = []\n",
        "    for topic_id in range(num_topics):\n",
        "        topic = lsi.print_topic(topic_id, num_keywords)\n",
        "        topic_keywords = [word.split(\"*\")[1].strip() for word in topic.split(\"+\")]  #Extract words from gensim output.\n",
        "        keywords.extend(topic_keywords)\n",
        "\n",
        "    return keywords\n",
        "\n",
        "\n",
        "def get_ngrams(tokens, n):\n",
        "  \"\"\"Generates n-grams from a list of tokens.\"\"\"\n",
        "  ngrams = []\n",
        "  for i in range(len(tokens) - n + 1):\n",
        "    ngrams.append(tuple(tokens[i:i+n]))\n",
        "  return ngrams\n",
        "\n",
        "\n",
        "def main():\n",
        "    search_term = input(\"Enter a Wikipedia search term: \")\n",
        "    try:\n",
        "        page = wikipedia.page(search_term)\n",
        "        text = page.content\n",
        "\n",
        "        #Extract bigrams\n",
        "        sentences = sent_tokenize(text)\n",
        "        all_tokens = []\n",
        "        for sentence in sentences:\n",
        "            tokens = word_tokenize(sentence.lower())\n",
        "            tokens = [w for w in tokens if w.isalnum() and w not in stopwords.words('english')]\n",
        "            all_tokens.extend(tokens)\n",
        "        bigrams = get_ngrams(all_tokens, 2)\n",
        "\n",
        "        print(\"\\nBigrams (2,2) ngrams:\")\n",
        "        print(bigrams[:10])\n",
        "\n",
        "        keywords = extract_lsi_keywords(text)\n",
        "        print(\"\\nLSI Keywords:\", keywords[:10])\n",
        "\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        print(f\"Error: Wikipedia page not found for '{search_term}'\")\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        print(f\"Error: Disambiguation error for '{search_term}'.  Possible options: {e.options}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDOmKf6KtZSN",
        "outputId": "72049043-cee9-43c6-f7c2-aacd334bd8f0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a Wikipedia search term: Amitabh Bachan \n",
            "\n",
            "Bigrams (2,2) ngrams:\n",
            "[('amitabh', 'bachchan'), ('bachchan', 'pronounced'), ('pronounced', 'ˈbətːʃən'), ('ˈbətːʃən', 'born'), ('born', 'amitabh'), ('amitabh', 'srivastava'), ('srivastava', '11'), ('11', 'october'), ('october', '1942'), ('1942', 'indian')]\n",
            "\n",
            "LSI Keywords: ['\"bachchan\"', '\"film\"', '\"amitabh\"', '\"actor\"', '\"best\"', '\"film\"', '\"bachchan\"', '\"best\"', '\"actor\"', '\"award\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "sentences = [\n",
        "    \"This is sentence 1 with 2023 and January.\",\n",
        "    \"Another sentence with some special characters like !@#$%^&*()_-+={}[]:;\\\"'<,>.?/~`.\",\n",
        "    \"Sentence 3.\",\n",
        "]\n",
        "\n",
        "months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "\n",
        "processed_sentences = [\n",
        "    \" \".join([word for word in sentence.lower().split()\n",
        "              if not any(char in punctuation for char in word) and not word.isdigit() and word not in months])\n",
        "    for sentence in sentences\n",
        "]\n",
        "\n",
        "print(processed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7SMw-0D7pIy",
        "outputId": "52d7ce84-515f-486b-ae4c-0807fc273bef"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this is sentence with and', 'another sentence with some special characters like', 'sentence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Awae_dcVxfkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ASckGmakxHSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uUdO5QVCv3Mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OvWkgGeevJnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YXFxzFICtZCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gXPnrNKdrxX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yz_ydXp2nwCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BECBl9Alm8ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ri-xlGuomZVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aUq7VtyLlZii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uLeLopoAk3Id"
      }
    }
  ]
}
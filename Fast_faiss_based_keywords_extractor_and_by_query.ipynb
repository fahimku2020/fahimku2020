{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObpyLnz3t2TbNbOrx04Xac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahimku2020/fahimku2020/blob/main/Fast_faiss_based_keywords_extractor_and_by_query.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1aBnWROu8ic",
        "outputId": "b2a321e2-ca96-449c-d6bc-64c408d83e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=c81b02e34fa9c97aaf46ce23054fb4b401195b28b999bf5dbc54503f299fe4b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia\n",
        "!pip install sentence_transformers\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "import numpy as np\n",
        "import nltk\n",
        "import torch\n",
        "from typing import List, Dict\n",
        "from functools import lru_cache\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download ('stopwords')\n",
        "\n",
        "class AdvancedKeywordExtractor:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2', language='english'):\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.stop_words = set(stopwords.words(language))\n",
        "\n",
        "    @lru_cache(maxsize=100)\n",
        "    def fetch_wikipedia_content(self, topic: str) -> str:\n",
        "        try:\n",
        "            page = wikipedia.page(topic)\n",
        "            return page.content\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching content: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        text = ''.join(char.lower() for char in text if char.isalnum() or char.isspace())\n",
        "        return text\n",
        "\n",
        "    def split_into_sentences(self, text: str) -> List[str]:\n",
        "        return nltk.sent_tokenize(text)\n",
        "\n",
        "    def generate_ngrams(self, text: str, n: int = 2) -> List[str]:\n",
        "        tokens = text.split()\n",
        "        filtered_tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        return [' '.join(gram) for gram in list(ngrams(filtered_tokens, n))]\n",
        "\n",
        "    def max_sum_keyword_selection(self, keywords: List[str], embeddings: np.ndarray, top_k: int = 5) -> List[str]:\n",
        "        if len(keywords) <= top_k:\n",
        "            return keywords\n",
        "\n",
        "        similarity_matrix = cosine_similarity(embeddings)\n",
        "        selected = [0]\n",
        "\n",
        "        while len(selected) < top_k:\n",
        "            diversity_scores = []\n",
        "            for i in range(len(keywords)):\n",
        "                if i not in selected:\n",
        "                    min_sim = min(similarity_matrix[i][j] for j in selected)\n",
        "                    diversity_scores.append(min_sim)\n",
        "                else:\n",
        "                    diversity_scores.append(-1)\n",
        "\n",
        "            next_keyword = np.argmax(diversity_scores)\n",
        "            selected.append(next_keyword)\n",
        "\n",
        "        return [keywords[i] for i in selected]\n",
        "\n",
        "    def extract_keywords_with_clustering(self, topic: str, n_clusters: int = 5, top_k_sentences: int = 3) -> Dict:\n",
        "        content = self.fetch_wikipedia_content(topic)\n",
        "        sentences = self.split_into_sentences(content)\n",
        "        clean_sentences = [self.clean_text(sent) for sent in sentences]\n",
        "\n",
        "        sentence_embeddings = self.model.encode(clean_sentences)\n",
        "\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(sentence_embeddings)\n",
        "\n",
        "        cluster_results = {}\n",
        "\n",
        "        for cluster in range(n_clusters):\n",
        "            cluster_mask = (cluster_labels == cluster)\n",
        "            cluster_sentences = [clean_sentences[i] for i in range(len(clean_sentences)) if cluster_mask[i]]\n",
        "            cluster_sentence_embeddings = sentence_embeddings[cluster_mask]\n",
        "\n",
        "            potential_keywords = []\n",
        "            for sent in cluster_sentences:\n",
        "                potential_keywords.extend(self.generate_ngrams(sent))\n",
        "\n",
        "            potential_keywords = list(set(potential_keywords))\n",
        "            keyword_embeddings = self.model.encode(potential_keywords)\n",
        "\n",
        "            selected_keywords = self.max_sum_keyword_selection(potential_keywords, keyword_embeddings)\n",
        "\n",
        "            keyword_relevant_sentences = {}\n",
        "            for keyword, keyword_emb in zip(selected_keywords, keyword_embeddings):\n",
        "                similarities = cosine_similarity(keyword_emb.reshape(1, -1), cluster_sentence_embeddings)[0]\n",
        "\n",
        "                top_sentence_indices = similarities.argsort()[-top_k_sentences:][::-1]\n",
        "                top_sentences = [\n",
        "                    {\n",
        "                        'sentence': cluster_sentences[idx],\n",
        "                        'similarity_score': similarities[idx]\n",
        "                    }\n",
        "                    for idx in top_sentence_indices\n",
        "                ]\n",
        "\n",
        "                keyword_relevant_sentences[keyword] = top_sentences\n",
        "\n",
        "            cluster_results[cluster] = {\n",
        "                'keywords': selected_keywords,\n",
        "                'relevant_sentences': keyword_relevant_sentences\n",
        "            }\n",
        "\n",
        "        return cluster_results\n",
        "\n",
        "    def find_most_relevant_cluster(self, results: Dict, user_query: str) -> Dict:\n",
        "        query_embedding = self.model.encode([user_query])[0]\n",
        "\n",
        "        cluster_similarities = {}\n",
        "        for cluster, data in results.items():\n",
        "            keywords = data['keywords']\n",
        "            keyword_embeddings = self.model.encode(keywords)\n",
        "\n",
        "            # Compute average similarity between query and cluster keywords\n",
        "            cluster_sim = np.mean([cosine_similarity(query_embedding.reshape(1, -1), ke.reshape(1, -1))[0][0]\n",
        "                                   for ke in keyword_embeddings])\n",
        "            cluster_similarities[cluster] = cluster_sim\n",
        "\n",
        "        # Find cluster with highest similarity\n",
        "        most_relevant_cluster = max(cluster_similarities, key=cluster_similarities.get)\n",
        "        return results[most_relevant_cluster]\n",
        "\n",
        "def main():\n",
        "    extractor = AdvancedKeywordExtractor()\n",
        "    topic = input(\"Enter Wikipedia topic to analyze: \")\n",
        "\n",
        "    # Extract keywords and clusters\n",
        "    results = extractor.extract_keywords_with_clustering(topic)\n",
        "\n",
        "    # User query for cluster retrieval\n",
        "    user_query = input(\"Enter a query to find relevant cluster: \")\n",
        "\n",
        "    # Find most relevant cluster based on user query\n",
        "    relevant_cluster = extractor.find_most_relevant_cluster(results, user_query)\n",
        "\n",
        "    print(\"\\nMost Relevant Cluster:\")\n",
        "    print(\"Keywords:\", relevant_cluster['keywords'])\n",
        "\n",
        "    print(\"\\nRelevant Sentences:\")\n",
        "    for keyword, sentences in relevant_cluster['relevant_sentences'].items():\n",
        "        print(f\"\\nKeyword: {keyword}\")\n",
        "        for sent_data in sentences:\n",
        "            print(f\"  Sentence: {sent_data['sentence']}\")\n",
        "            print(f\"  Highest Similarity Score: {sent_data['similarity_score']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTfcVT2Su9Wh",
        "outputId": "a7295c5e-959c-4020-d531-c781d93d13d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Wikipedia topic to analyze: Amitabh Bachan \n",
            "Enter a query to find relevant cluster: Films\n",
            "\n",
            "Most Relevant Cluster:\n",
            "Keywords: ['bachchan family', 'family delhi', 'family also', 'family choose', '2013 family']\n",
            "\n",
            "Relevant Sentences:\n",
            "\n",
            "Keyword: bachchan family\n",
            "  Sentence: the bachchan family also bought shares worth 252000 in meridian tech a consulting company in the us\n",
            "  Highest Similarity Score: 0.5040\n",
            "  Sentence: he and his family choose to stay away from the limelight\n",
            "  Highest Similarity Score: 0.3647\n",
            "  Sentence: in 2013 he and his family donated 25 million 42664 to a charitable trust plan india that works for the betterment of young girls in india\n",
            "  Highest Similarity Score: 0.2691\n",
            "\n",
            "Keyword: family delhi\n",
            "  Sentence: despite significant expectations it had poor returns at the box office\n",
            "  Highest Similarity Score: 0.5965\n",
            "  Sentence: the fiasco and the consequent legal battles surrounding abcl and various entities after the event coupled with the fact that abcl was reported to have overpaid most of its toplevel managers eventually led to its financial and operational collapse in 1997\n",
            "  Highest Similarity Score: 0.3125\n",
            "  Sentence: the bungalow is worth 500 million\n",
            "  Highest Similarity Score: 0.2839\n",
            "\n",
            "Keyword: family also\n",
            "  Sentence: despite significant expectations it had poor returns at the box office\n",
            "  Highest Similarity Score: 0.2520\n",
            "  Sentence: he has hosted several seasons of the game show kaun banega crorepati indias version of the game show franchise who wants to be a millionaire\n",
            "  Highest Similarity Score: 0.2518\n",
            "  Sentence: the girls hang on to him with incredulous desperation and he bats for them with all he has\n",
            "  Highest Similarity Score: 0.2404\n",
            "\n",
            "Keyword: family choose\n",
            "  Sentence: he then returned to host the fourth season and has hosted the show since\n",
            "  Highest Similarity Score: 0.6052\n",
            "  Sentence: the show was well received\n",
            "  Highest Similarity Score: 0.4207\n",
            "  Sentence: the fifth season started on 15 august 2011 and ended on 17 november 2011\n",
            "  Highest Similarity Score: 0.3851\n",
            "\n",
            "Keyword: 2013 family\n",
            "  Sentence: taking advantage of this resurgence amitabh began endorsing a variety of products and services appearing in many television and billboard advertisements\n",
            "  Highest Similarity Score: 0.2178\n",
            "  Sentence: abcls strategy was to introduce products and services covering an entire crosssection of indias entertainment industry\n",
            "  Highest Similarity Score: 0.2115\n",
            "  Sentence: he has been a vocal brand ambassador of the swachh bharat mission sbm and featured in a few advertisements to promote the campaign\n",
            "  Highest Similarity Score: 0.1516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "23vrIN5xyfIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhVc0ik3yngG",
        "outputId": "33e0755a-31ca-4e9f-f421-9a2697c631bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/27.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/27.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:07\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/27.5 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/27.5 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/27.5 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/27.5 MB\u001b[0m \u001b[31m133.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m20.1/27.5 MB\u001b[0m \u001b[31m158.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m25.2/27.5 MB\u001b[0m \u001b[31m149.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m148.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m148.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m148.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "import numpy as np\n",
        "import nltk\n",
        "import faiss\n",
        "from typing import List, Dict\n",
        "from functools import lru_cache\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class AdvancedKeywordExtractor:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2', language='english', batch_size=32):\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.stop_words = set(stopwords.words(language))\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    @lru_cache(maxsize=100)\n",
        "    def fetch_wikipedia_content(self, topic: str) -> str:\n",
        "        try:\n",
        "            page = wikipedia.page(topic)\n",
        "            return page.content\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching content: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        return ''.join(char.lower() for char in text if char.isalnum() or char.isspace())\n",
        "\n",
        "    def split_into_sentences(self, text: str) -> List[str]:\n",
        "        return nltk.sent_tokenize(text)\n",
        "\n",
        "    def generate_ngrams(self, text: str, n: int = 2) -> List[str]:\n",
        "        tokens = text.split()\n",
        "        filtered_tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        return [' '.join(gram) for gram in list(ngrams(filtered_tokens, n))]\n",
        "\n",
        "    def batch_encode(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Encode texts in batches for memory efficiency\"\"\"\n",
        "        embeddings = []\n",
        "        for i in range(0, len(texts), self.batch_size):\n",
        "            batch = texts[i:i+self.batch_size]\n",
        "            batch_embeddings = self.model.encode(batch)\n",
        "            embeddings.append(batch_embeddings)\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    def max_sum_keyword_selection(self, keywords: List[str], embeddings: np.ndarray, top_k: int = 5) -> List[str]:\n",
        "        if len(keywords) <= top_k:\n",
        "            return keywords\n",
        "\n",
        "        similarity_matrix = cosine_similarity(embeddings)\n",
        "        selected = [0]\n",
        "\n",
        "        while len(selected) < top_k:\n",
        "            diversity_scores = []\n",
        "            for i in range(len(keywords)):\n",
        "                if i not in selected:\n",
        "                    min_sim = min(similarity_matrix[i][j] for j in selected)\n",
        "                    diversity_scores.append(min_sim)\n",
        "                else:\n",
        "                    diversity_scores.append(-1)\n",
        "\n",
        "            next_keyword = np.argmax(diversity_scores)\n",
        "            selected.append(next_keyword)\n",
        "\n",
        "        return [keywords[i] for i in selected]\n",
        "\n",
        "    def create_faiss_index(self, embeddings: np.ndarray):\n",
        "        \"\"\"Create FAISS index for efficient similarity search\"\"\"\n",
        "        index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "        index.add(embeddings)\n",
        "        return index\n",
        "\n",
        "    def extract_keywords_with_clustering(self, topic: str, n_clusters: int = 10, top_k_sentences: int = 3) -> Dict:\n",
        "        content = self.fetch_wikipedia_content(topic)\n",
        "        sentences = self.split_into_sentences(content)\n",
        "        clean_sentences = [self.clean_text(sent) for sent in sentences]\n",
        "\n",
        "        sentence_embeddings = self.batch_encode(clean_sentences)\n",
        "\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(sentence_embeddings)\n",
        "\n",
        "        cluster_results = {}\n",
        "\n",
        "        for cluster in range(n_clusters):\n",
        "            cluster_mask = (cluster_labels == cluster)\n",
        "            cluster_sentences = [clean_sentences[i] for i in range(len(clean_sentences)) if cluster_mask[i]]\n",
        "            cluster_sentence_embeddings = sentence_embeddings[cluster_mask]\n",
        "\n",
        "            potential_keywords = []\n",
        "            for sent in cluster_sentences:\n",
        "                potential_keywords.extend(self.generate_ngrams(sent))\n",
        "\n",
        "            potential_keywords = list(set(potential_keywords))\n",
        "            keyword_embeddings = self.batch_encode(potential_keywords)\n",
        "\n",
        "            # Create FAISS index for efficient similarity search\n",
        "            faiss_index = self.create_faiss_index(cluster_sentence_embeddings)\n",
        "\n",
        "            selected_keywords = self.max_sum_keyword_selection(potential_keywords, keyword_embeddings)\n",
        "\n",
        "            keyword_relevant_sentences = {}\n",
        "            for keyword, keyword_emb in zip(selected_keywords, keyword_embeddings):\n",
        "                # Use FAISS for similarity search\n",
        "                _, indices = faiss_index.search(keyword_emb.reshape(1, -1), top_k_sentences)\n",
        "\n",
        "                top_sentences = [\n",
        "                    {\n",
        "                        'sentence': cluster_sentences[idx],\n",
        "                        'similarity_score': cosine_similarity(keyword_emb.reshape(1, -1),\n",
        "                                                             cluster_sentence_embeddings[idx].reshape(1, -1))[0][0]\n",
        "                    }\n",
        "                    for idx in indices[0]\n",
        "                ]\n",
        "\n",
        "                keyword_relevant_sentences[keyword] = top_sentences\n",
        "\n",
        "            cluster_results[cluster] = {\n",
        "                'keywords': selected_keywords,\n",
        "                'sentences': cluster_sentences,\n",
        "                'relevant_sentences': keyword_relevant_sentences\n",
        "            }\n",
        "\n",
        "        return cluster_results\n",
        "\n",
        "    def find_most_relevant_cluster(self, results: Dict, user_query: str) -> Dict:\n",
        "        query_embedding = self.model.encode([user_query])[0]\n",
        "\n",
        "        cluster_similarities = {}\n",
        "        for cluster, data in results.items():\n",
        "            keywords = data['keywords']\n",
        "            keyword_embeddings = self.batch_encode(keywords)\n",
        "\n",
        "            cluster_sim = np.mean([cosine_similarity(query_embedding.reshape(1, -1), ke.reshape(1, -1))[0][0]\n",
        "                                   for ke in keyword_embeddings])\n",
        "            cluster_similarities[cluster] = cluster_sim\n",
        "\n",
        "        most_relevant_cluster = max(cluster_similarities, key=cluster_similarities.get)\n",
        "        return results[most_relevant_cluster]\n",
        "\n",
        "def main():\n",
        "    extractor = AdvancedKeywordExtractor()\n",
        "\n",
        "    # Get Wikipedia topic\n",
        "    topic = input(\"Enter Wikipedia topic to analyze: \")\n",
        "\n",
        "    # Extract keywords and clusters\n",
        "    results = extractor.extract_keywords_with_clustering(topic)\n",
        "\n",
        "    # Print all cluster information\n",
        "    print(\"\\nAll Cluster Details:\")\n",
        "    for cluster, data in results.items():\n",
        "        print(f\"\\nCluster {cluster}:\")\n",
        "        print(\"Keywords:\", data['keywords'])\n",
        "        print(\"\\nCluster Sentences:\")\n",
        "        for sent in data['sentences'][:5]:  # Print first 3 sentences\n",
        "            print(f\"  - {sent}\")\n",
        "\n",
        "    # User query for cluster retrieval\n",
        "    user_query = input(\"\\nEnter a query to find relevant cluster: \")\n",
        "\n",
        "    # Find most relevant cluster based on user query\n",
        "    relevant_cluster = extractor.find_most_relevant_cluster(results, user_query)\n",
        "\n",
        "    print(\"\\nMost Relevant Cluster:\")\n",
        "    print(\"Keywords:\", relevant_cluster['keywords'])\n",
        "\n",
        "    print(\"\\nRelevant Sentences:\")\n",
        "    for keyword, sentences in relevant_cluster['relevant_sentences'].items():\n",
        "        print(f\"\\nKeyword: {keyword}\")\n",
        "        for sent_data in sentences:\n",
        "            print(f\"  Sentence: {sent_data['sentence'][:200]}\")\n",
        "            print(f\"  Highest Similarity Score: {sent_data['similarity_score']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx_iqzpf9-Ys",
        "outputId": "ce89b26e-e15e-40c7-e0ae-71a592363a05"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Wikipedia topic to analyze: Amitabh Bachan \n",
            "\n",
            "All Cluster Details:\n",
            "\n",
            "Cluster 0:\n",
            "Keywords: ['bachchans character', 'bachchan nearfatal', 'coolie bachchan', 'coolie bachchans', 'amitabh bachchan']\n",
            "\n",
            "Cluster Sentences:\n",
            "  - it is thought that his mother might have had some influence on his choice of career for she always insisted that he should take centre stage\n",
            "  - however they were struggling to find an actor for the lead angry young man role it was turned down by several actors owing to it going against the romantic hero image dominant in the industry at the time\n",
            "  - on 26 july 1982 while filming a fight scene with coactor puneet issar for coolie bachchan had a nearfatal intestinal injury\n",
            "  - however as he jumped towards the table the corner of the table struck his abdomen resulting in a splenic rupture from which he lost a significant amount of blood\n",
            "  - the director manmohan desai altered the ending of coolie bachchans character was originally intended to have been killed off but after the change of script the character lived in the end\n",
            "\n",
            "Cluster 1:\n",
            "Keywords: ['police chief', 'chief investigated', 'bureau investigation', 'indian investigators', 'case court']\n",
            "\n",
            "Cluster Sentences:\n",
            "  - bachchan has also been involved in several humanitarian works and he is a leading brand endorser in india\n",
            "  - in october 2003 time magazine dubbed bachchan the star of the millennium\n",
            "  - teji bachchan was a punjabi sikh khatri from lyallpur punjab british india presentday faisalabad punjab pakistan\n",
            "  - bachchans father died in 2003 and his mother in 2007\n",
            "  - this led to bachchan being dubbed as the angry young man a journalistic catchphrase that became a metaphor for the dormant rage frustration restlessness sense of rebellion and antiestablishment disposition of an entire generation prevalent in 1970s india\n",
            "\n",
            "Cluster 2:\n",
            "Keywords: ['times india', 'india time', 'hindustan times', 'indian year', 'history indian']\n",
            "\n",
            "Cluster Sentences:\n",
            "  - bachchan is often hailed as the shahenshah of bollywood sadi ke mahanayak translated as greatest actor of the century in hindi star of the millennium or simply big b\n",
            "  - bachchan also made an appearance in a hollywood film the great gatsby 2013 in which he played a nonindian jewish character\n",
            "  - in addition to acting bachchan has worked as a playback singer film producer and television presenter\n",
            "  - bachchan was voted the greatest star of stage or screen in the bbc your millennium online poll in 1999\n",
            "  - when bachchan finished his studies his father approached prithviraj kapoor the founder of prithvi theatre and patriarch of the kapoor acting family to see if there was an opening for him but kapoor offered no encouragement\n",
            "\n",
            "Cluster 3:\n",
            "Keywords: ['performance piku', 'piku without', 'piku fourth', 'phalke award', 'kong 2011']\n",
            "\n",
            "Cluster Sentences:\n",
            "  - with a cinematic career spanning over five decades he has played pivotal roles in over 200 films\n",
            "  - he is a recipient of several accolades including six national film awards and sixteen filmfare awards\n",
            "  - for piku he won his fourth national film award for best actor making him the only actor to do so\n",
            "  - bachchan has won numerous accolades in his career including record four national film awards in best actor category and many awards at international film festivals and award ceremonies\n",
            "  - he has won sixteen filmfare awards and is the most nominated performer in any major acting category at filmfare with 34 nominations in best actor and 42 nominations overall\n",
            "\n",
            "Cluster 4:\n",
            "Keywords: ['people western', 'western world', 'dubbed indias', 'yash chopras', 'history indian']\n",
            "\n",
            "Cluster Sentences:\n",
            "  - he is often considered one of the greatest most accomplished and commercially successful actors in the history of indian cinema\n",
            "  - he first gained popularity in the early1970s for films such as anand zanjeer and roti kapada aur makaan and achieved greater stardom in later years being dubbed indias angry young man for several of his onscreen roles in hindi films\n",
            "  - he consistently starred in topgrossing indian films from the mid1970s to the 1980s such as deewaar sholay kabhi kabhie hera pheri amar akbar anthony parvarish kasme vaade don trishul muqaddar ka sikandar suhaag dostana naseeb laawaris namak halaal andhaa kaanoon coolie sharaabi and mard as well as some of his most acclaimed performances include namak haraam abhimaan majboor mili chupke chupke do anjaane kaala patthar shaan silsila yaarana kaalia shakti aakhree raasta shahenshah and agneepath\n",
            "  - he became a business executive for bird  company in kolkata calcutta  and worked in theatre before starting his film career\n",
            "  - during this time he made a guest appearance in the film guddi which starred his future wife jaya bhaduri\n",
            "\n",
            "Cluster 5:\n",
            "Keywords: ['expressing concern', 'concern new', 'excellent response', 'appearing many', 'received stating']\n",
            "\n",
            "Cluster Sentences:\n",
            "  - after taking a break from acting in the 1990s his resurgence was marked in 2000 with mohabbatein\n",
            "  - the film was a huge success and one of the highestgrossing films of that year breaking bachchans dry spell at the box office and making him a star\n",
            "  - the film opened to excellent response all over the country eventually taking top spot at the box office that year and emerging an all time blockbuster as well as bachchans biggest hit at that point of time\n",
            "  - the film was the highestgrossing film of that year\n",
            "  - nevertheless he resumed filming later that year after a long period of recuperation\n",
            "\n",
            "Cluster 6:\n",
            "Keywords: ['trishul kaala', 'deewaar trishul', 'mala sinha', 'sarkar kabhi', 'babli sarkar']\n",
            "\n",
            "Cluster Sentences:\n",
            "  - his film career started in 1969 as a voice narrator in mrinal sens film bhuvan shome\n",
            "  - since then he starred in several successful and acclaimed films like kabhi khushi kabhie gham aankhen baghban khakee black bunty aur babli sarkar kabhi alvida naa kehna bhoothnath cheeni kum paa piku pink badla brahmāstra part one  shiva and kalki 2898 ad\n",
            "  - his first acting role was as one of the seven protagonists in the film saat hindustani directed by khwaja ahmad abbas and featuring utpal dutt anwar ali brother of comedian mehmood madhu and jalal agha\n",
            "  - he then played his first antagonist role as an infatuated loverturnedmurderer in parwana 1971\n",
            "  - following parwana were several films including reshma aur shera 1971\n",
            "\n",
            "Cluster 7:\n",
            "Keywords: ['given 250000', '250000 4678', '250 million', 'million 42664', '25 million']\n",
            "\n",
            "Cluster Sentences:\n",
            "  - he was educated at sherwood college nainital and kirori mal college university of delhi\n",
            "  - he has hosted several seasons of the game show kaun banega crorepati indias version of the game show franchise who wants to be a millionaire\n",
            "  - he also entered politics for a time in the 1980s\n",
            "  - beyond the indian subcontinent he acquired a large overseas following of the south asian diaspora as well as others in markets including africa south africa eastern africa and mauritius the middle east especially egypt and the uae the united kingdom russia central asia the caribbean guyana suriname and trinidad and tobago oceania fiji australia and new zealand canada and the united states\n",
            "  - he attended kirori mal college at the university of delhi in delhi\n",
            "\n",
            "Cluster 8:\n",
            "Keywords: ['honoured highest', 'honour conferred', 'honour 2007', 'honour officer', 'legion honour']\n",
            "\n",
            "Cluster Sentences:\n",
            "  - his dominance in the indian film industry during the 1970s80s led the french director françois truffaut to describe it as a oneman industry\n",
            "  - the government of france honoured him with its highest civilian honour officer of the legion of honour in 2007 for his exceptional career in the world of cinema and beyond\n",
            "  - french director françois truffaut called him a oneman industry\n",
            "  - in 2003 he was conferred with the honorary citizenship of the french town of deauville\n",
            "  - frances highest civilian honour the officer of the legion of honour was conferred upon him by the french government in 2007 for his exceptional career in the world of cinema and beyond\n",
            "\n",
            "Cluster 9:\n",
            "Keywords: ['veteran actress', 'actress politician', 'married actress', 'actress aishwarya', 'aishwarya rai']\n",
            "\n",
            "Cluster Sentences:\n",
            "  - amitabh bachchan pronounced əmɪˈtɑːbʱ ˈbətːʃən  born amitabh srivastava 11 october 1942 is an indian actor who works in hindi cinema\n",
            "  - amitabh bachchan was born in 1942 in allahabad now prayagraj to the hindi poet harivansh rai bachchan and his wife the social activist teji bachchan\n",
            "  -  early life and family \n",
            "\n",
            "bachchan was born on 11 october 1942 in allahabad now prayagraj to hindi poet harivansh rai bachchan and social activist teji bachchan\n",
            "  - harivansh rai bachchan was an awadhi hindu kayastha who was fluent in awadhi hindi and urdu\n",
            "  - harivanshs ancestors came from a village called babupatti in the raniganj tehsil in the pratapgarh district in the presentday state of uttar pradesh in india\n",
            "\n",
            "Enter a query to find relevant cluster: Amitabh Bachan performance and awards in his life\n",
            "\n",
            "Most Relevant Cluster:\n",
            "Keywords: ['bachchans character', 'bachchan nearfatal', 'coolie bachchan', 'coolie bachchans', 'amitabh bachchan']\n",
            "\n",
            "Relevant Sentences:\n",
            "\n",
            "Keyword: bachchans character\n",
            "  Sentence: the director manmohan desai altered the ending of coolie bachchans character was originally intended to have been killed off but after the change of script the character lived in the end\n",
            "  Highest Similarity Score: 0.5475\n",
            "  Sentence: according to raja sen of rediffcom amitabh bachchan a retired lawyer with bipolar disorder takes up cudgels on behalf of the girls delivering courtroom blows with pugilistic grace\n",
            "  Highest Similarity Score: 0.4583\n",
            "  Sentence: on 26 july 1982 while filming a fight scene with coactor puneet issar for coolie bachchan had a nearfatal intestinal injury\n",
            "  Highest Similarity Score: 0.4184\n",
            "\n",
            "Keyword: bachchan nearfatal\n",
            "  Sentence: his histrionics come primarily in the form of his wellmodulated baritone conveying his emotions and of course from the wellwritten lines\n",
            "  Highest Similarity Score: 0.6674\n",
            "  Sentence: it is thought that his mother might have had some influence on his choice of career for she always insisted that he should take centre stage\n",
            "  Highest Similarity Score: 0.1445\n",
            "  Sentence: according to raja sen of rediffcom amitabh bachchan a retired lawyer with bipolar disorder takes up cudgels on behalf of the girls delivering courtroom blows with pugilistic grace\n",
            "  Highest Similarity Score: 0.1210\n",
            "\n",
            "Keyword: coolie bachchan\n",
            "  Sentence: however they were struggling to find an actor for the lead angry young man role it was turned down by several actors owing to it going against the romantic hero image dominant in the industry at the t\n",
            "  Highest Similarity Score: 0.4192\n",
            "  Sentence: the footage of the fight scene is frozen at the critical moment and a caption appears onscreen marking it as the instant of the actors injury\n",
            "  Highest Similarity Score: 0.3656\n",
            "  Sentence: on 26 july 1982 while filming a fight scene with coactor puneet issar for coolie bachchan had a nearfatal intestinal injury\n",
            "  Highest Similarity Score: 0.3079\n",
            "\n",
            "Keyword: coolie bachchans\n",
            "  Sentence: it is thought that his mother might have had some influence on his choice of career for she always insisted that he should take centre stage\n",
            "  Highest Similarity Score: 0.3861\n",
            "  Sentence: at one point to ward off a possible suitor he casually mentions that his daughter isnt a virgin that shes financially independent and sexually independent too\n",
            "  Highest Similarity Score: 0.2034\n",
            "  Sentence: he bickers with the maids harrows his hapless helper and expects piku to stay unmarried so she can attend to him\n",
            "  Highest Similarity Score: 0.1907\n",
            "\n",
            "Keyword: amitabh bachchan\n",
            "  Sentence: he and his family choose to stay away from the limelight\n",
            "  Highest Similarity Score: 0.5478\n",
            "  Sentence: according to raja sen of rediffcom amitabh bachchan a retired lawyer with bipolar disorder takes up cudgels on behalf of the girls delivering courtroom blows with pugilistic grace\n",
            "  Highest Similarity Score: 0.0895\n",
            "  Sentence: the next year he played the role of a grumpy father experiencing chronic constipation in the critically acclaimed piku which was also one of the biggest hits of 2015\n",
            "  Highest Similarity Score: 0.0855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yMSTY-mo99_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CpLpSybEyfrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nilWv_Vv8LmY"
      }
    }
  ]
}
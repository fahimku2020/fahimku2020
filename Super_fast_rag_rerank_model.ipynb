{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObhPTQcOFXq5X6g/nO29iP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahimku2020/fahimku2020/blob/main/Super_fast_rag_rerank_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build simple rag in python without open api key,fetch data from wikipedia generate user input question and generate answers of different paragraphs based on semantic clustering ,rerank top answers based on similarity score, apply semantic text splitting,fast its execution speed by optimization"
      ],
      "metadata": {
        "id": "hx3LB79U6p24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A3K44jy-6pn9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP_k7xKEkBq9",
        "outputId": "e967e6e1-63d9-4bff-8c4a-34f314a4aa52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from BeautifulSoup4) (2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=b3df7f309fe426b01d30ae0e85401c1241faa9f9613f0c9c83420c2c26253d47\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install BeautifulSoup4\n",
        "!pip  install requests\n",
        "!pip install faiss-cpu\n",
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List, Tuple\n",
        "\n",
        "class WikipediaRAG:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
        "        \"\"\"\n",
        "        Initialize the RAG system with a sentence embedding model\n",
        "\n",
        "        :param model_name: Sentence transformer model for embeddings\n",
        "        \"\"\"\n",
        "        # Load sentence embedding model\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "        # Initialize Faiss index for semantic search\n",
        "        self.dimension = self.embedding_model.get_sentence_embedding_dimension()\n",
        "        self.index = faiss.IndexFlatL2(self.dimension)\n",
        "\n",
        "        # Storage for text passages and their metadata\n",
        "        self.passages = []\n",
        "        self.page_titles = []\n",
        "\n",
        "    def semantic_text_split(self, text: str, max_length: int = 250) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text semantically into overlapping chunks\n",
        "\n",
        "        :param text: Input text to split\n",
        "        :param max_length: Maximum length of each passage\n",
        "        :return: List of text passages\n",
        "        \"\"\"\n",
        "        # Split text into sentences\n",
        "        sentences = text.split('. ')\n",
        "        passages = []\n",
        "        current_passage = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Add sentence to current passage\n",
        "            current_passage.append(sentence)\n",
        "            current_length += len(sentence)\n",
        "\n",
        "            # If passage is too long, create a new passage\n",
        "            if current_length > max_length:\n",
        "                passages.append('. '.join(current_passage))\n",
        "                current_passage = current_passage[-2:]  # Overlap with previous context\n",
        "                current_length = len('. '.join(current_passage))\n",
        "\n",
        "        # Add remaining passage\n",
        "        if current_passage:\n",
        "            passages.append('. '.join(current_passage))\n",
        "\n",
        "        return passages\n",
        "\n",
        "    def fetch_and_process_wikipedia(self, topic: str):\n",
        "        \"\"\"\n",
        "        Fetch Wikipedia page, split into semantic passages, and index\n",
        "\n",
        "        :param topic: Wikipedia topic to retrieve\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Fetch Wikipedia page\n",
        "            page = wikipedia.page(topic)\n",
        "\n",
        "            # Semantic text splitting\n",
        "            passages = self.semantic_text_split(page.content)\n",
        "\n",
        "            # Generate embeddings for passages\n",
        "            embeddings = self.embedding_model.encode(passages)\n",
        "\n",
        "            # Add to Faiss index\n",
        "            self.index.add(embeddings)\n",
        "\n",
        "            # Store passages and titles for reference\n",
        "            self.passages.extend(passages)\n",
        "            self.page_titles.extend([page.title] * len(passages))\n",
        "\n",
        "            print(f\"Processed {topic}: {len(passages)} passages\")\n",
        "\n",
        "        except wikipedia.exceptions.DisambiguationError as e:\n",
        "            print(f\"Multiple matches for {topic}. Suggestions: {e.options}\")\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            print(f\"No Wikipedia page found for {topic}\")\n",
        "\n",
        "    def retrieve_top_passages(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Retrieve top passages based on semantic similarity\n",
        "\n",
        "        :param query: User query\n",
        "        :param top_k: Number of top passages to retrieve\n",
        "        :return: List of tuples (passage, similarity_score)\n",
        "        \"\"\"\n",
        "        # Embed query\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "\n",
        "        # Search in Faiss index\n",
        "        distances, indices = self.index.search(query_embedding, top_k)\n",
        "\n",
        "        # Sort and return top passages with their similarity scores\n",
        "        results = [\n",
        "            (self.passages[idx], 1 / (1 + dist))  # Convert distance to similarity score\n",
        "            for idx, dist in zip(indices[0], distances[0])\n",
        "        ]\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_answer(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate an answer by retrieving and synthesizing top passages\n",
        "\n",
        "        :param query: User query\n",
        "        :return: Generated answer\n",
        "        \"\"\"\n",
        "        # Retrieve top passages\n",
        "        top_passages = self.retrieve_top_passages(query)\n",
        "\n",
        "        # Synthesize answer from top passages\n",
        "        context = \"\\n\".join([passage for passage, _ in top_passages])\n",
        "\n",
        "        # Simple answer generation (can be replaced with more advanced LLM)\n",
        "        answer = f\"Based on the context from Wikipedia:\\n\\n{context}\"\n",
        "\n",
        "        return answer\n",
        "\n",
        "def main():\n",
        "    # Create RAG instance\n",
        "    rag = WikipediaRAG()\n",
        "\n",
        "    # Fetch and process some initial topics\n",
        "    topics = ['Artificial Intelligence', 'Machine Learning', 'Python Programming','Amitabh bachan' ]\n",
        "    for topic in topics:\n",
        "        rag.fetch_and_process_wikipedia(topic)\n",
        "\n",
        "    # Interactive loop\n",
        "    while True:\n",
        "        query = input(\"\\nEnter your question (or 'exit' to quit): \")\n",
        "\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Generate and print answer\n",
        "        answer = rag.generate_answer(query)\n",
        "        print(\"\\nAnswer:\", answer)\n",
        "\n",
        "        # Print source passages with similarity scores\n",
        "        print(\"\\nTop Relevant Passages:\")\n",
        "        top_passages = rag.retrieve_top_passages(query)\n",
        "        for passage, score in top_passages:\n",
        "            print(f\"Similarity Score: {score:.2f}\")\n",
        "            print(passage[:300] + \"...\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "64f247ypnUuT",
        "outputId": "4bdcf3dc-2747-482a-c50f-74124f033cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Artificial Intelligence: 383 passages\n",
            "No Wikipedia page found for Machine Learning\n",
            "Processed Python Programming: 206 passages\n",
            "Processed Amitabh bachan: 261 passages\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7f6201686ed7>\u001b[0m in \u001b[0;36m<cell line: 153>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-7f6201686ed7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# Interactive loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter your question (or 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimized rag model"
      ],
      "metadata": {
        "id": "I8EIbb_hsb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BmV8exvDsbr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhhj6QZYsuRO",
        "outputId": "c6066580-719a-4a9c-b468-755d91dceb65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.8.30)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14346 sha256=791a231cba52d6f72af4d8a96ff7f8e36e05e208f2d638d1ef5ab370c6ba5d29\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5x6bUfRcsuBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import wikipediaapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "class OptimizedWikipediaRAG:\n",
        "    def __init__(self,\n",
        "                 chunk_size: int = 100,\n",
        "                 top_k: int = 5,\n",
        "                 language: str = 'en'):\n",
        "        \"\"\"\n",
        "        Initialize RAG system with optimization parameters\n",
        "\n",
        "        Args:\n",
        "            chunk_size: Number of tokens per text chunk\n",
        "            top_k: Number of top results to return\n",
        "            language: Wikipedia language edition\n",
        "        \"\"\"\n",
        "        # Lightweight model for fast embedding\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Wikipedia API client\n",
        "        self.wiki = wikipediaapi.Wikipedia(\n",
        "            language=language,\n",
        "            extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
        "            user_agent='MyWikipediaApp/1.0 (my_email@example.com)'\n",
        "        )\n",
        "\n",
        "        # Optimization parameters\n",
        "        self.chunk_size = chunk_size\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def semantic_text_split(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into semantically meaningful chunks\n",
        "\n",
        "        Args:\n",
        "            text: Input text to split\n",
        "\n",
        "        Returns:\n",
        "            List of text chunks\n",
        "        \"\"\"\n",
        "        # Simple splitting with semantic awareness\n",
        "        words = text.split()\n",
        "        chunks = [\n",
        "            ' '.join(words[i:i+self.chunk_size])\n",
        "            for i in range(0, len(words), self.chunk_size)\n",
        "        ]\n",
        "        return chunks\n",
        "\n",
        "    def fetch_wikipedia_content(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Fetch Wikipedia page content\n",
        "\n",
        "        Args:\n",
        "            query: Search query\n",
        "\n",
        "        Returns:\n",
        "            Extracted page content\n",
        "        \"\"\"\n",
        "        try:\n",
        "            page = self.wiki.page(query)\n",
        "            return page.text if page.exists() else \"\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching Wikipedia content: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def embed_chunks(self, chunks: List[str]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate embeddings for text chunks\n",
        "\n",
        "        Args:\n",
        "            chunks: List of text chunks\n",
        "\n",
        "        Returns:\n",
        "            Embedding matrix\n",
        "        \"\"\"\n",
        "        # Parallel embedding for speed\n",
        "        return self.model.encode(chunks, show_progress_bar=False)\n",
        "\n",
        "    def rerank_results(self,\n",
        "                       query: str,\n",
        "                       chunks: List[str],\n",
        "                       embeddings: np.ndarray) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Rerank results based on semantic similarity\n",
        "\n",
        "        Args:\n",
        "            query: User query\n",
        "            chunks: Text chunks\n",
        "            embeddings: Chunk embeddings\n",
        "\n",
        "        Returns:\n",
        "            Reranked results with scores\n",
        "        \"\"\"\n",
        "        # Embed query\n",
        "        query_embedding = self.model.encode([query])[0]\n",
        "\n",
        "        # Compute cosine similarities\n",
        "        similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
        "\n",
        "        # Sort and select top results\n",
        "        ranked_results = sorted(\n",
        "            [\n",
        "                {\n",
        "                    'chunk': chunk,\n",
        "                    'similarity': sim\n",
        "                }\n",
        "                for chunk, sim in zip(chunks, similarities)\n",
        "            ],\n",
        "            key=lambda x: x['similarity'],\n",
        "            reverse=True\n",
        "        )[:self.top_k]\n",
        "\n",
        "        return ranked_results\n",
        "\n",
        "    def answer_query(self, query: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Main method to process query and return results\n",
        "\n",
        "        Args:\n",
        "            query: User input query\n",
        "\n",
        "        Returns:\n",
        "            Reranked answer chunks\n",
        "        \"\"\"\n",
        "        # Fetch Wikipedia content\n",
        "        content = self.fetch_wikipedia_content(query.split()[0])\n",
        "\n",
        "        # Semantic text splitting\n",
        "        chunks = self.semantic_text_split(content)\n",
        "\n",
        "        # Generate embeddings\n",
        "        embeddings = self.embed_chunks(chunks)\n",
        "\n",
        "        # Rerank results\n",
        "        return self.rerank_results(query, chunks, embeddings)\n",
        "\n",
        "def main():\n",
        "    # Example usage\n",
        "    rag = OptimizedWikipediaRAG(chunk_size=50, top_k=2)\n",
        "\n",
        "    # Measure execution time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\"computer science \",\"solar system\",\"politics\"\n",
        "    ]\n",
        "\n",
        "    # Process multiple queries\n",
        "    for query in tqdm(queries, desc=\"Processing Queries\"):\n",
        "        results = rag.answer_query(query)\n",
        "\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        for i, result in enumerate(results, 1):\n",
        "            print(f\"Result {i}:\")\n",
        "            print(f\"Similarity: {result['similarity']:.4f}\")\n",
        "            print(f\"Chunk: {result['chunk'][:1000]}...\\n\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Total Execution Time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TzrMuEzsXRo",
        "outputId": "65716e1e-2236-4613-8809-7f70947cb039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Queries:  33%|███▎      | 1/3 [00:09<00:19,  9.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: computer science \n",
            "Result 1:\n",
            "Similarity: 0.4620\n",
            "Chunk: any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity. Artificial intelligence A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in...\n",
            "\n",
            "Result 2:\n",
            "Similarity: 0.4554\n",
            "Chunk: typical modern definition of a computer is: \"A device that computes, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information.\" According to this definition, any device that processes information qualifies as a computer. Future There is active...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Queries:  67%|██████▋   | 2/3 [00:10<00:04,  4.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: solar system\n",
            "Result 1:\n",
            "Similarity: 0.5653\n",
            "Chunk: eclipse, an eclipse of a sun in which it is obstructed by the moon Solar System, the planetary system made up by the Sun and the objects orbiting it Solar Maximum Mission, a satellite SOLAR (ISS), an observatory on International Space Station Music \"Solar\" (composition), attributed to Miles Davis Solar...\n",
            "\n",
            "Result 2:\n",
            "Similarity: 0.5578\n",
            "Chunk: Solar may refer to: Astronomy Of or relating to the Sun Solar telescope, a special purpose telescope used to observe the Sun A device that utilizes solar energy (e.g. \"solar panels\") Solar calendar, a calendar whose dates indicate the position of the Earth on its revolution around the Sun Solar...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Queries: 100%|██████████| 3/3 [00:17<00:00,  5.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: politics\n",
            "Result 1:\n",
            "Similarity: 0.5965\n",
            "Chunk: Politics (from Ancient Greek πολιτικά (politiká) 'affairs of the cities') is the set of activities that are associated with making decisions in groups, or other forms of power relations among individuals, such as the distribution of status or resources. The branch of social science that studies politics and government is...\n",
            "\n",
            "Result 2:\n",
            "Similarity: 0.5221\n",
            "Chunk: ('rule of thieves'). Insincere politics The words \"politics\" and \"political\" are sometimes used as pejoratives to mean political action that is deemed to be overzealous, performative, or insincere. Levels of politics Macropolitics Macropolitics can either describe political issues that affect an entire political system (e.g. the nation state), or refer...\n",
            "\n",
            "Total Execution Time: 17.34 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "caching techniques"
      ],
      "metadata": {
        "id": "t-V-Nz0p0SER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install diskcache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x42oyVIJ0eDg",
        "outputId": "fb126612-8e4c-4888-ed79-4f09f7df4785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting diskcache\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache\n",
            "Successfully installed diskcache-5.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class WikipediaRAG:\n",
        "    def __init__(self,\n",
        "                 embedding_model: str = 'all-MiniLM-L6-v2',\n",
        "                 max_paragraphs: int = 20,\n",
        "                 clustering_method: str = 'kmeans'):\n",
        "        \"\"\"\n",
        "        Initialize RAG system with semantic embedding and clustering capabilities\n",
        "\n",
        "        Args:\n",
        "            embedding_model (str): Sentence transformer model for embeddings\n",
        "            max_paragraphs (int): Maximum number of paragraphs to process\n",
        "            clustering_method (str): Clustering approach ('kmeans')\n",
        "        \"\"\"\n",
        "        # Use SentenceTransformer for efficient semantic embeddings\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        self.max_paragraphs = max_paragraphs\n",
        "        self.clustering_method = clustering_method\n",
        "\n",
        "    def fetch_wikipedia_content(self, topic: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Fetch and preprocess Wikipedia content\n",
        "\n",
        "        Args:\n",
        "            topic (str): Wikipedia search topic\n",
        "\n",
        "        Returns:\n",
        "            List[str]: Preprocessed paragraphs\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Fetch Wikipedia page\n",
        "            page = wikipedia.page(topic)\n",
        "\n",
        "            # Split content into paragraphs\n",
        "            paragraphs = page.content.split('\\n\\n')\n",
        "\n",
        "            # Filter and preprocess paragraphs\n",
        "            paragraphs = [\n",
        "                p.strip() for p in paragraphs\n",
        "                if p.strip() and len(p.split()) > 10\n",
        "            ][:self.max_paragraphs]\n",
        "\n",
        "            return paragraphs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching Wikipedia content: {e}\")\n",
        "            return []\n",
        "\n",
        "    def semantic_text_splitting(self, text: str, chunk_size: int = 100) -> List[str]:\n",
        "        \"\"\"\n",
        "        Advanced semantic text splitting with overlapping\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            chunk_size (int): Number of tokens per chunk\n",
        "\n",
        "        Returns:\n",
        "            List[str]: Semantically split text chunks\n",
        "        \"\"\"\n",
        "        tokens = text.split()\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(tokens), chunk_size // 2):\n",
        "            chunk = ' '.join(tokens[i:i+chunk_size])\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def embed_paragraphs(self, paragraphs: List[str]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate embeddings for paragraphs\n",
        "\n",
        "        Args:\n",
        "            paragraphs (List[str]): Input paragraphs\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Paragraph embeddings\n",
        "        \"\"\"\n",
        "        return self.embedding_model.encode(paragraphs, show_progress_bar=False)\n",
        "\n",
        "    def cluster_paragraphs(self, embeddings: np.ndarray, n_clusters: int = 3) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Cluster paragraphs based on semantic similarity\n",
        "\n",
        "        Args:\n",
        "            embeddings (np.ndarray): Paragraph embeddings\n",
        "            n_clusters (int): Number of semantic clusters\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Cluster labels\n",
        "        \"\"\"\n",
        "        kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "        return kmeans.fit_predict(embeddings)\n",
        "\n",
        "    def semantic_search(self, query: str, paragraphs: List[str], embeddings: np.ndarray) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Perform semantic search and ranking\n",
        "\n",
        "        Args:\n",
        "            query (str): User query\n",
        "            paragraphs (List[str]): Source paragraphs\n",
        "            embeddings (np.ndarray): Paragraph embeddings\n",
        "\n",
        "        Returns:\n",
        "            List[Tuple[str, float]]: Ranked paragraphs with similarity scores\n",
        "        \"\"\"\n",
        "        # Embed query\n",
        "        query_embedding = self.embedding_model.encode([query])[0]\n",
        "\n",
        "        # Compute cosine similarities\n",
        "        similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
        "\n",
        "        # Create ranked list of paragraphs\n",
        "        ranked_paragraphs = sorted(\n",
        "            zip(paragraphs, similarities),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        return ranked_paragraphs\n",
        "\n",
        "    def generate_answer(self,\n",
        "                        query: str,\n",
        "                        topic: str,\n",
        "                        top_k: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate comprehensive answer using RAG approach\n",
        "\n",
        "        Args:\n",
        "            query (str): User query\n",
        "            topic (str): Wikipedia topic\n",
        "            top_k (int): Number of top paragraphs to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Dict containing answer details\n",
        "        \"\"\"\n",
        "        # Fetch and preprocess paragraphs\n",
        "        paragraphs = self.fetch_wikipedia_content(topic)\n",
        "\n",
        "        if not paragraphs:\n",
        "            return {\"error\": \"No content found\"}\n",
        "\n",
        "        # Generate embeddings\n",
        "        embeddings = self.embed_paragraphs(paragraphs)\n",
        "\n",
        "        # Semantic clustering\n",
        "        cluster_labels = self.cluster_paragraphs(embeddings)\n",
        "\n",
        "        # Semantic search and re-ranking\n",
        "        ranked_paragraphs = self.semantic_search(query, paragraphs, embeddings)\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"topic\": topic,\n",
        "            \"top_answers\": ranked_paragraphs[:top_k],\n",
        "            \"clusters\": cluster_labels.tolist()\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    # Example usage\n",
        "    rag_system = WikipediaRAG()\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"What is the history of artificial intelligence?\",\n",
        "        \"Explain quantum computing basics\",\n",
        "        \"Tell me about climate change impact\", \"Filmfare awards\"\n",
        "    ]\n",
        "\n",
        "    topics = [\n",
        "        \"Artificial Intelligence\",\n",
        "        \"Quantum Computing\",\n",
        "        \"Climate Change\",\"Amitabh Bachan \"\n",
        "    ]\n",
        "\n",
        "    for query, topic in zip(queries, topics):\n",
        "        result = rag_system.generate_answer(query, topic)\n",
        "\n",
        "        print(\"\\n--- Results ---\")\n",
        "        print(f\"Query: {result['query']}\")\n",
        "        print(f\"Topic: {result['topic']}\")\n",
        "\n",
        "        print(\"\\nTop Answers:\")\n",
        "        for i, (paragraph, score) in enumerate(result['top_answers'], 1):\n",
        "            print(f\"{i}. Score: {score:.4f}\")\n",
        "            print(f\"   {paragraph[:300]}...\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teTOno2T5LD7",
        "outputId": "af88b613-c531-4bb0-b44d-56ba85b8f829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Results ---\n",
            "Query: What is the history of artificial intelligence?\n",
            "Topic: Artificial Intelligence\n",
            "\n",
            "Top Answers:\n",
            "1. Score: 0.5528\n",
            "   === General intelligence ===\n",
            "A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence....\n",
            "\n",
            "2. Score: 0.5370\n",
            "   Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence...\n",
            "\n",
            "3. Score: 0.5228\n",
            "   == Techniques ==\n",
            "AI research uses a wide variety of techniques to accomplish the goals above....\n",
            "\n",
            "\n",
            "--- Results ---\n",
            "Query: Explain quantum computing basics\n",
            "Topic: Quantum Computing\n",
            "\n",
            "Top Answers:\n",
            "1. Score: 0.7129\n",
            "   A quantum computer is a computer that exploits quantum mechanical phenomena. On small scales, physical matter exhibits properties of both particles and waves, and quantum computing leverages this behavior using specialized hardware. Classical physics cannot explain the operation of these quantum dev...\n",
            "\n",
            "2. Score: 0.6632\n",
            "   There are a number of models of computation for quantum computing, distinguished by the basic elements in which the computation is decomposed....\n",
            "\n",
            "3. Score: 0.6517\n",
            "   Computer engineers typically describe a modern computer's operation in terms of classical electrodynamics.\n",
            "Within these \"classical\" computers, some components (such as semiconductors and random number generators) may rely on quantum behavior, but these components are not isolated from their environm...\n",
            "\n",
            "\n",
            "--- Results ---\n",
            "Query: Tell me about climate change impact\n",
            "Topic: Climate Change\n",
            "\n",
            "Top Answers:\n",
            "1. Score: 0.6226\n",
            "   Present-day climate change includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth's climate. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global temperatures is driven by hum...\n",
            "\n",
            "2. Score: 0.5779\n",
            "   The environmental effects of climate change are broad and far-reaching, affecting oceans, ice, and weather. Changes may occur gradually or rapidly. Evidence for these effects comes from studying climate change in the past, from modelling, and from modern observations. Since the 1950s, droughts and h...\n",
            "\n",
            "3. Score: 0.5630\n",
            "   Recent warming has driven many terrestrial and freshwater species poleward and towards higher altitudes. For instance, the range of hundreds of North American birds has shifted northward at an average rate of 1.5 km/year over the past 55 years. Higher atmospheric CO2 levels and an extended growing s...\n",
            "\n",
            "\n",
            "--- Results ---\n",
            "Query: Filmfare awards\n",
            "Topic: Amitabh Bachan \n",
            "\n",
            "Top Answers:\n",
            "1. Score: 0.4810\n",
            "   === Return to success (2000–present) ===\n",
            "In 2000, Bachchan appeared in Aditya Chopra's romantic blockbuster Mohabbatein. He played a stern, elder figure who rivalled the character of Shahrukh Khan. His role won him his third Filmfare Award for Best Supporting Actor. Other hits followed, with Bachcha...\n",
            "\n",
            "2. Score: 0.4768\n",
            "   In 1976, he was cast by Yash Chopra in the romantic musical Kabhi Kabhie. Bachchan starred as a young poet, Amit Malhotra, who falls deeply in love with a beautiful young girl named Pooja (Rakhee Gulzar) who ends up marrying someone else (Shashi Kapoor). The film was notable for portraying Bachchan ...\n",
            "\n",
            "3. Score: 0.4735\n",
            "   === Career fluctuations, sabbatical, business ventures and acting comeback (1989–1999) ===\n",
            "After the success of his comeback film however, Bachchan's star power began to wane as his subsequent releases like Gangaa Jamunaa Saraswati, Jaadugar, Toofan and Main Azaad Hoon (all released in 1989) did not...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1PbpcuNj5K2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T-m3QUIosW-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fny5h45mnUhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1pNOzuwqmslG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0ztoE8Dmk5uE"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlo+sqUH1B3/ZqYpSP8CJa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahimku2020/fahimku2020/blob/main/Fast_while_clusterand_query_based_keywords_extractor_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFAJyqY-JWNT",
        "outputId": "8feee2a6-cfb8-4424-8381-aa17c7569401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-gpu\n",
        "!pip install sentence_transformers\n",
        "!pip install  wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "import wikipedia\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "import unicodedata\n",
        "import functools\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "class KeywordExtractor:\n",
        "    def __init__(self, topic, num_clusters=5):\n",
        "        # Initialize models and parameters\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.num_clusters = num_clusters\n",
        "        self.topic = topic\n",
        "\n",
        "        # Fetch and preprocess document\n",
        "        self.document = self.fetch_wikipedia_document()\n",
        "        self.sentences = sent_tokenize(self.document)\n",
        "        self.clean_sentences = [self.clean_text(sent) for sent in self.sentences]\n",
        "\n",
        "        # Compute embeddings\n",
        "        self.embeddings = self.model.encode(self.clean_sentences)\n",
        "\n",
        "        # Clustering\n",
        "        self.clusters = self.perform_clustering()\n",
        "\n",
        "        # Extract keywords\n",
        "        self.keywords_per_cluster = self.extract_keywords_per_cluster()\n",
        "\n",
        "        # Create FAISS index for fast similarity search\n",
        "        self.faiss_index = self.create_faiss_index()\n",
        "\n",
        "    def fetch_wikipedia_document(self):\n",
        "        \"\"\"Fetch Wikipedia document for the given topic.\"\"\"\n",
        "        try:\n",
        "            page = wikipedia.page(self.topic)\n",
        "            return page.content\n",
        "        except wikipedia.exceptions.DisambiguationError as e:\n",
        "            print(f\"Multiple matches found. Using first option: {e.options[0]}\")\n",
        "            page = wikipedia.page(e.options[0])\n",
        "            return page.content\n",
        "\n",
        "    @functools.lru_cache(maxsize=1000)\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Preprocess and clean text.\"\"\"\n",
        "        # Remove special characters and digits\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove extra whitespaces\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Remove stopwords\n",
        "        words = [word for word in text.split() if word not in self.stop_words]\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def perform_clustering(self):\n",
        "        \"\"\"Perform K-means clustering on sentence embeddings.\"\"\"\n",
        "        kmeans = KMeans(n_clusters=self.num_clusters, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(self.embeddings)\n",
        "        return cluster_labels\n",
        "\n",
        "    def extract_keywords_per_cluster(self):\n",
        "        \"\"\"Extract keywords for each cluster using n-gram.\"\"\"\n",
        "        vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
        "\n",
        "        keywords_per_cluster = {}\n",
        "        for cluster in range(self.num_clusters):\n",
        "            cluster_sentences = [\n",
        "                sent for sent, label in zip(self.clean_sentences, self.clusters)\n",
        "                if label == cluster\n",
        "            ]\n",
        "\n",
        "            # Vectorize cluster sentences\n",
        "            if cluster_sentences:\n",
        "                vectorized = vectorizer.fit_transform(cluster_sentences)\n",
        "                keywords = vectorizer.get_feature_names_out()\n",
        "                keywords_per_cluster[cluster] = keywords\n",
        "\n",
        "        return keywords_per_cluster\n",
        "\n",
        "    def create_faiss_index(self):\n",
        "        \"\"\"Create FAISS index for efficient similarity search.\"\"\"\n",
        "        index = faiss.IndexFlatL2(self.embeddings.shape[1])\n",
        "        index.add(self.embeddings)\n",
        "        return index\n",
        "\n",
        "    def find_most_relevant_sentences(self):\n",
        "        \"\"\"Find most relevant sentences for each cluster.\"\"\"\n",
        "        cluster_sentences = {}\n",
        "        for cluster in range(self.num_clusters):\n",
        "            # Get cluster sentences\n",
        "            cluster_mask = self.clusters == cluster\n",
        "            cluster_sent_embeddings = self.embeddings[cluster_mask]\n",
        "            cluster_sent_texts = [\n",
        "                sent for sent, label in zip(self.clean_sentences, self.clusters)\n",
        "                if label == cluster\n",
        "            ]\n",
        "\n",
        "            # Compute max-sum diversity\n",
        "            selected_indices = self.max_sum_diversity(cluster_sent_embeddings)\n",
        "\n",
        "            # Store results\n",
        "            cluster_sentences[cluster] = {\n",
        "                'keywords': self.keywords_per_cluster.get(cluster, []),\n",
        "                'sentences': [cluster_sent_texts[idx] for idx in selected_indices]\n",
        "            }\n",
        "\n",
        "        return cluster_sentences\n",
        "\n",
        "    def max_sum_diversity(self, embeddings, num_sentences=3):\n",
        "        \"\"\"Select diverse sentences using max-sum method.\"\"\"\n",
        "        if len(embeddings) <= num_sentences:\n",
        "            return list(range(len(embeddings)))\n",
        "\n",
        "        selected_indices = [0]  # Start with first sentence\n",
        "\n",
        "        while len(selected_indices) < num_sentences:\n",
        "            remaining_indices = list(set(range(len(embeddings))) - set(selected_indices))\n",
        "\n",
        "            # Compute diversity score\n",
        "            diversity_scores = []\n",
        "            for idx in remaining_indices:\n",
        "                candidate_embedding = embeddings[idx]\n",
        "                min_distance = np.min([\n",
        "                    np.linalg.norm(candidate_embedding - embeddings[sel_idx])\n",
        "                    for sel_idx in selected_indices\n",
        "                ])\n",
        "                diversity_scores.append(min_distance)\n",
        "\n",
        "            # Select most diverse sentence\n",
        "            best_idx = remaining_indices[np.argmax(diversity_scores)]\n",
        "            selected_indices.append(best_idx)\n",
        "\n",
        "        return selected_indices\n",
        "\n",
        "def main():\n",
        "    # User input for topic\n",
        "    topic = input(\"Enter Wikipedia topic for keyword extraction: \")\n",
        "\n",
        "    # Initialize extractor\n",
        "    extractor = KeywordExtractor(topic)\n",
        "\n",
        "    # Find and print cluster sentences\n",
        "    cluster_results = extractor.find_most_relevant_sentences()\n",
        "\n",
        "    print(\"\\n--- Cluster Analysis ---\")\n",
        "    for cluster, data in cluster_results.items():\n",
        "        print(f\"\\nCluster {cluster}:\")\n",
        "        print(\"Keywords:\", data['keywords'][:5])\n",
        "        print(\"Representative Sentences:\")\n",
        "        for sent in data['sentences']:\n",
        "            print(f\"  - {sent}\")\n",
        "\n",
        "    # Interactive query\n",
        "    while True:\n",
        "        query = input(\"\\nEnter query to search clusters (or 'exit' to quit): \")\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Find most similar cluster to query\n",
        "        query_embedding = extractor.model.encode([extractor.clean_text(query)])[0]\n",
        "        distances, indices = extractor.faiss_index.search(\n",
        "            query_embedding.reshape(1, -1), k=5\n",
        "        )\n",
        "\n",
        "        most_similar_cluster = extractor.clusters[indices[0][0]]\n",
        "\n",
        "        print(f\"\\nMost Relevant Cluster (Cluster {most_similar_cluster}):\")\n",
        "        print(\"Keywords:\", cluster_results[most_similar_cluster]['keywords'][:5])\n",
        "        print(\"Representative Sentences:\")\n",
        "        for sent in cluster_results[most_similar_cluster]['sentences'][:10 ]:\n",
        "            print(f\"  - {sent}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmjAaWxiJW-o",
        "outputId": "ffee32c8-7e50-4789-d72d-507ca052b83a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Wikipedia topic for keyword extraction: Amitabh Bachan \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cluster Analysis ---\n",
            "\n",
            "Cluster 0:\n",
            "Keywords: ['abcl main' 'abcl reported' 'abcl various' 'abcls operations'\n",
            " 'abcls strategy']\n",
            "Representative Sentences:\n",
            "  - taking break acting resurgence marked mohabbatein\n",
            "  - june became first living asian modelled wax londons madame tussauds wax museum\n",
            "  - girls hang incredulous desperation bats\n",
            "\n",
            "Cluster 1:\n",
            "Keywords: ['ab legend' 'abcl event' 'abhishek actor' 'according raja'\n",
            " 'accused using']\n",
            "Representative Sentences:\n",
            "  - amitabh bachchan born allahabad prayagraj hindi poet harivansh rai bachchan wife social activist teji bachchan\n",
            "  - bachchan named panama papers paradise papers leaked confidential documents relating offshore investment\n",
            "  - writing ndtv troy ribeiro indoasian news service ians stated amitabh bachchan deepak sehgall aged defence lawyer shines always restrained powerful performance\n",
            "\n",
            "Cluster 2:\n",
            "Keywords: ['aag released' 'aaj ka' 'aamir khan' 'aankhen baghban' 'aankhen kaante']\n",
            "Representative Sentences:\n",
            "  - educated sherwood college nainital kirori mal college university delhi\n",
            "  - major hits year include parvarish khoon pasina\n",
            "  - salim khan wrote story screenplay script zanjeer conceived angry young man persona lead role\n",
            "\n",
            "Cluster 3:\n",
            "Keywords: ['accident emerged' 'accolades career' 'acted anjaane' 'acting bachchan'\n",
            " 'acting career']\n",
            "Representative Sentences:\n",
            "  - amitabh bachchan pronounced mtb btn born amitabh srivastava october indian actor works hindi cinema\n",
            "  - however struggling find actor lead angry young man role turned several actors owing going romantic hero image dominant industry time\n",
            "  - second season followed run cut short star plus bachchan fell ill\n",
            "\n",
            "Cluster 4:\n",
            "Keywords: ['aakhree raasta' 'abhimaan majboor' 'acclaim commercial'\n",
            " 'acclaim filmfare' 'acclaimed performances']\n",
            "Representative Sentences:\n",
            "  - often considered one greatest accomplished commercially successful actors history indian cinema\n",
            "  - film premiered toronto international film festival september\n",
            "  - government india awarded padma shri padma bhushan padma vibhushan dadasaheb phalke award\n",
            "\n",
            "Enter query to search clusters (or 'exit' to quit): Amitabh Bachan films names\n",
            "\n",
            "Most Relevant Cluster (Cluster 3):\n",
            "Keywords: ['accident emerged' 'accolades career' 'acted anjaane' 'acting bachchan'\n",
            " 'acting career']\n",
            "Representative Sentences:\n",
            "  - amitabh bachchan pronounced mtb btn born amitabh srivastava october indian actor works hindi cinema\n",
            "  - however struggling find actor lead angry young man role turned several actors owing going romantic hero image dominant industry time\n",
            "  - second season followed run cut short star plus bachchan fell ill\n",
            "\n",
            "Enter query to search clusters (or 'exit' to quit): Career\n",
            "\n",
            "Most Relevant Cluster (Cluster 0):\n",
            "Keywords: ['abcl main' 'abcl reported' 'abcl various' 'abcls operations'\n",
            " 'abcls strategy']\n",
            "Representative Sentences:\n",
            "  - taking break acting resurgence marked mohabbatein\n",
            "  - june became first living asian modelled wax londons madame tussauds wax museum\n",
            "  - girls hang incredulous desperation bats\n",
            "\n",
            "Enter query to search clusters (or 'exit' to quit): Amitabh Bachan entered into politics \n",
            "\n",
            "Most Relevant Cluster (Cluster 3):\n",
            "Keywords: ['accident emerged' 'accolades career' 'acted anjaane' 'acting bachchan'\n",
            " 'acting career']\n",
            "Representative Sentences:\n",
            "  - amitabh bachchan pronounced mtb btn born amitabh srivastava october indian actor works hindi cinema\n",
            "  - however struggling find actor lead angry young man role turned several actors owing going romantic hero image dominant industry time\n",
            "  - second season followed run cut short star plus bachchan fell ill\n"
          ]
        }
      ]
    }
  ]
}
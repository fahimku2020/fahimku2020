{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahimku2020/fahimku2020/blob/main/Best_keyword_extraction_clutering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO9vJADAV1Wd",
        "outputId": "9bc8727c-e31c-4048-ece2-0d68710f7a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=9c5f5b97527f505ddeb66a30bf2660467c4e60060b1106e24e72b88051417fef\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zJnCyGi3hD_",
        "outputId": "1972200b-83f3-4975-d847-32a60d59b1a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keywords for 'Amitabh Bachan': ['the filmfare', 'the filmfare', 'the filmfare', 'the filmfare', 'the filmfare', 'the filmfare', 'with kapoor', 'filmfare with', 'bollywood in', 'with filmfare']\n"
          ]
        }
      ],
      "source": [
        "import wikipedia\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import  torch\n",
        "import  numpy as np\n",
        "\n",
        "nltk.download('punkt') # Download Punkt Sentence Tokenizer if you haven't already\n",
        "\n",
        "\n",
        "def fetch_document(page_title):\n",
        "    \"\"\"Fetches a document from Wikipedia.\"\"\"\n",
        "    try:\n",
        "        page = wikipedia.page(page_title)\n",
        "        return page.content\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        return \"Page not found.\"\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        return f\"Disambiguation error: {e.options}\"\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans the text by removing HTML tags, non-alphanumeric characters, and lowercasing.\"\"\"\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text()\n",
        "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', clean_text)  #Remove punctuation\n",
        "    clean_text = clean_text.lower()\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    \"\"\"Splits the text into sentences.\"\"\"\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "\n",
        "def tokenize_ngram(text, n=2):\n",
        "    \"\"\"Tokenizes the text using n-grams.\"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    ngrams = list(nltk.ngrams(tokens, n))\n",
        "    return [\" \".join(gram) for gram in ngrams]\n",
        "\n",
        "\n",
        "def max_sum_similarity(sentences, top_n=5):\n",
        "  \"\"\"\n",
        "  Extracts keywords using Max Sum Similarity.  This is a simplified version;\n",
        "  a more robust implementation would involve more sophisticated similarity metrics\n",
        "  and handling of sentence weighting.\n",
        "  \"\"\"\n",
        "  model = SentenceTransformer('all-mpnet-base-v2') # You can experiment with other models\n",
        "  embeddings = model.encode(sentences)\n",
        "  similarities = util.cos_sim(embeddings, embeddings)\n",
        "\n",
        "  scores = similarities.sum(axis=0)\n",
        "  score_np = scores.cpu().numpy() #Get indices of top N sentences\n",
        "\n",
        "  top_indices_np = np.flip (score_np.argsort()[-top_n :])\n",
        "\n",
        "  top_indices = top_indices_np.tolist()\n",
        "\n",
        "  keywords = [sentences[i] for i in top_indices]\n",
        "  return keywords\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "page_title = \"Amitabh Bachan\"  #Change to your desired Wikipedia page\n",
        "document = fetch_document(page_title)\n",
        "\n",
        "if document != \"Page not found.\" :\n",
        "    clean_doc = clean_text(document)\n",
        "    sentences = split_into_sentences(clean_doc)\n",
        "\n",
        "    #Only get bigrams from the first 10 sentences to keep processing time reasonable\n",
        "    bigrams_all = []\n",
        "    for sent in sentences[:100]:\n",
        "      bigrams_all.extend(tokenize_ngram(sent))\n",
        "\n",
        "    keywords = max_sum_similarity(bigrams_all, top_n=10)\n",
        "    print(f\"Keywords for '{page_title}': {keywords}\")\n",
        "\n",
        "else:\n",
        "    print(document)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-3AXJRF3gvd"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH98jPjx3go1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XBnBYpnZ7cp",
        "outputId": "c502ab82-ae8a-410c-c469-9ae10b698137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keywords: ['bachchan first national film', 'film award best actor', 'amitabh bachchan biggest film', 'bachchan started amitabh bachchan', 'actor award role film', 'bachchan first filmfare award', 'career bachchan made film', 'award role film year', 'also year bachchan starred', 'filmfare awards amitabh bachchan']\n"
          ]
        }
      ],
      "source": [
        "import wikipedia\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.util import ngrams\n",
        "from string import punctuation\n",
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Download required NLTK data\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "#nltk.download('maxent_ne_chunker')\n",
        "#nltk.download('words')\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "class KeywordExtractor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.punctuation = set(punctuation)\n",
        "\n",
        "    def fetch_wikipedia_content(self, title):\n",
        "        \"\"\"\n",
        "        Fetch content from Wikipedia for a given title\n",
        "        \"\"\"\n",
        "        try:\n",
        "            page = wikipedia.page(title)\n",
        "            return page.content\n",
        "        except wikipedia.exceptions.DisambiguationError as e:\n",
        "            return wikipedia.page(e.options[0]).content\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            return None\n",
        "\n",
        "    def split_into_sentences(self, text):\n",
        "        \"\"\"\n",
        "        Split text into sentences using NLTK\n",
        "        \"\"\"\n",
        "        return sent_tokenize(text)\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"\n",
        "        Clean and preprocess text\n",
        "        \"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove special characters and digits\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Remove stopwords\n",
        "        words = word_tokenize(text)\n",
        "        words = [word for word in words if word not in self.stop_words]\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def generate_ngrams(self, text, n_range=(4, 4)):\n",
        "        \"\"\"\n",
        "        Generate n-grams from text\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(text)\n",
        "        all_ngrams = []\n",
        "\n",
        "        for n in range(n_range[0], n_range[1] + 1):\n",
        "            n_grams = list(ngrams(tokens, n))\n",
        "            all_ngrams.extend([' '.join(gram) for gram in n_grams])\n",
        "\n",
        "        return all_ngrams\n",
        "\n",
        "    def extract_noun_phrases(self, text):\n",
        "        \"\"\"\n",
        "        Extract noun phrases from text using POS tagging\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(text)\n",
        "        tagged = pos_tag(tokens)\n",
        "        noun_phrases = []\n",
        "\n",
        "        # Pattern for noun phrases (simple version: adjective? + noun+)\n",
        "        current_phrase = []\n",
        "        for word, tag in tagged:\n",
        "            if tag.startswith(('JJ', 'NN')):  # Adjectives and nouns\n",
        "                current_phrase.append(word)\n",
        "            else:\n",
        "                if current_phrase:\n",
        "                    noun_phrases.append(' '.join(current_phrase))\n",
        "                    current_phrase = []\n",
        "\n",
        "        if current_phrase:  # Add the last phrase if exists\n",
        "            noun_phrases.append(' '.join(current_phrase))\n",
        "\n",
        "        return noun_phrases\n",
        "\n",
        "    def max_sum_similarity(self, candidates, doc_embedding, top_n=30, nr_candidates=10):\n",
        "        \"\"\"\n",
        "        Select keywords using max sum similarity\n",
        "        \"\"\"\n",
        "        # Convert candidates to embeddings (using simple TF-IDF approach for demonstration)\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        candidate_embeddings = vectorizer.fit_transform(candidates).toarray()\n",
        "        doc_embedding = vectorizer.transform([doc_embedding]).toarray()[0]\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = np.dot(candidate_embeddings, doc_embedding)\n",
        "\n",
        "        # Get top candidates based on similarity\n",
        "        sorted_indices = np.argsort(similarities)[::-1]\n",
        "        top_candidates = [candidates[idx] for idx in sorted_indices[:nr_candidates]]\n",
        "\n",
        "        # Select diverse set of keywords\n",
        "        selected_keywords = []\n",
        "        for _ in range(top_n):\n",
        "            if not top_candidates:\n",
        "                break\n",
        "\n",
        "            # Select the candidate with highest similarity to document\n",
        "            selected = top_candidates[0]\n",
        "            selected_keywords.append(selected)\n",
        "\n",
        "            # Remove selected candidate and similar candidates\n",
        "            top_candidates = top_candidates[1:]\n",
        "\n",
        "        return selected_keywords\n",
        "\n",
        "    def extract_keywords(self, title, top_n=30):\n",
        "        \"\"\"\n",
        "        Main function to extract keywords from Wikipedia article\n",
        "        \"\"\"\n",
        "        # Fetch content\n",
        "        content = self.fetch_wikipedia_content(title)\n",
        "        if not content:\n",
        "            return []\n",
        "\n",
        "        # Split into sentences\n",
        "        sentences = self.split_into_sentences(content)\n",
        "\n",
        "        # Clean text\n",
        "        cleaned_text = self.clean_text(' '.join(sentences  ))\n",
        "\n",
        "        # Generate n-grams\n",
        "        all_ngrams = self.generate_ngrams(cleaned_text)\n",
        "\n",
        "        # Extract noun phrases\n",
        "        noun_phrases = self.extract_noun_phrases(cleaned_text)\n",
        "\n",
        "        # Combine n-grams and noun phrases\n",
        "        candidates = list(set(all_ngrams ))\n",
        "\n",
        "        # Select keywords using max sum similarity\n",
        "        keywords = self.max_sum_similarity(candidates, cleaned_text, top_n=top_n)\n",
        "\n",
        "        return keywords\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    extractor = KeywordExtractor()\n",
        "    keywords = extractor.extract_keywords(\"Amitabh  Bachan \", top_n=30)\n",
        "    print(\"Keywords:\", keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgFD-38Zg7y6"
      },
      "outputs": [],
      "source": [
        "def fetch_wikipedia_content(topic):\n",
        "  try:\n",
        "    page = wikipedia.page(topic)\n",
        "    return page.content\n",
        "  except Exception as e:\n",
        "    print(f\"Error fetching Wikipedia content: {e}\")\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6KcnG18k-63",
        "outputId": "60d494f8-3de7-435c-b2a1-5bc919f18234"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: \n",
            "    <p>Hey! This is a sample text ðŸ˜Š with HTML tags, emojis, and URLs https://example.com.\n",
            "    It also has some nÃºmeros (numbers) and \"special\" chars!!\n",
            "    Let's see how it's cleaned... We're testing contractions here.</p>\n",
            "    \n",
            "\n",
            "Cleaned text: hey sample text html tag emojis url also nmeros number special char let see cleaned testing contraction\n",
            "\n",
            "Custom cleaned text: hey this is a sample text with html tags emojis and urls it also has some nmeros numbers and special chars lets see how its cleaned were testing contractions here\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import string\n",
        "\n",
        "class TextCleaner:\n",
        "    def __init__(self, language='english'):\n",
        "        # Download required NLTK data\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('wordnet')\n",
        "\n",
        "        self.stop_words = set(stopwords.words(language))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def remove_html_tags(self, text):\n",
        "        \"\"\"Remove HTML tags from text\"\"\"\n",
        "        clean = re.compile('<.*?>')\n",
        "        return re.sub(clean, '', text)\n",
        "\n",
        "    def remove_urls(self, text):\n",
        "        \"\"\"Remove URLs from text\"\"\"\n",
        "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "        return url_pattern.sub('', text)\n",
        "\n",
        "    def remove_emoji(self, text):\n",
        "        \"\"\"Remove emojis from text\"\"\"\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "        return emoji_pattern.sub('', text)\n",
        "\n",
        "\n",
        "\n",
        "    def remove_special_characters(self, text, remove_digits=True):\n",
        "        \"\"\"Remove special characters and optionally digits from text\"\"\"\n",
        "        pattern = r'[^a-zA-Z\\s]' if remove_digits else r'[^a-zA-Z0-9\\s]'\n",
        "        return re.sub(pattern, '', text)\n",
        "\n",
        "    def remove_extra_whitespace(self, text):\n",
        "        \"\"\"Remove extra whitespace from text\"\"\"\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_stopwords(self, text):\n",
        "        \"\"\"Remove stopwords from text\"\"\"\n",
        "        words = word_tokenize(text)\n",
        "        return ' '.join([word for word in words if word.lower() not in self.stop_words])\n",
        "\n",
        "    def lemmatize_text(self, text):\n",
        "        \"\"\"Lemmatize text\"\"\"\n",
        "        words = word_tokenize(text)\n",
        "        return ' '.join([self.lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "    def stem_text(self, text):\n",
        "        \"\"\"Stem text\"\"\"\n",
        "        words = word_tokenize(text)\n",
        "        return ' '.join([self.stemmer.stem(word) for word in words])\n",
        "\n",
        "    def remove_punctuation(self, text):\n",
        "        \"\"\"Remove punctuation from text\"\"\"\n",
        "        translator = str.maketrans('', '', string.punctuation)\n",
        "        return text.translate(translator)\n",
        "\n",
        "    def convert_lowercase(self, text):\n",
        "        \"\"\"Convert text to lowercase\"\"\"\n",
        "        return text.lower()\n",
        "\n",
        "    def clean_text(self, text,\n",
        "                  lowercase=True,\n",
        "                  remove_html=True,\n",
        "                  remove_url=True,\n",
        "                  remove_emojis=True,\n",
        "                  expand_contractions=True,\n",
        "                  remove_accents=True,\n",
        "                  remove_special_chars=True,\n",
        "                  remove_digits=True,\n",
        "                  remove_punct=True,\n",
        "                  remove_stops=True,\n",
        "                  lemmatize=True,\n",
        "                  stem=False,\n",
        "                  extra_whitespace=True):\n",
        "        \"\"\"\n",
        "        Main function to clean text with configurable options\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text to clean\n",
        "            lowercase (bool): Convert to lowercase\n",
        "            remove_html (bool): Remove HTML tags\n",
        "            remove_url (bool): Remove URLs\n",
        "            remove_emojis (bool): Remove emojis\n",
        "            expand_contractions (bool): Expand contractions\n",
        "            remove_accents (bool): Remove accented characters\n",
        "            remove_special_chars (bool): Remove special characters\n",
        "            remove_digits (bool): Remove digits\n",
        "            remove_punct (bool): Remove punctuation\n",
        "            remove_stops (bool): Remove stopwords\n",
        "            lemmatize (bool): Lemmatize text\n",
        "            stem (bool): Stem text (not recommended with lemmatization)\n",
        "            extra_whitespace (bool): Remove extra whitespace\n",
        "\n",
        "        Returns:\n",
        "            str: Cleaned text\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Apply cleaning operations based on parameters\n",
        "        if remove_html:\n",
        "            text = self.remove_html_tags(text)\n",
        "\n",
        "        if remove_url:\n",
        "            text = self.remove_urls(text)\n",
        "\n",
        "        if remove_emojis:\n",
        "            text = self.remove_emoji(text)\n",
        "\n",
        "\n",
        "        if lowercase:\n",
        "            text = self.convert_lowercase(text)\n",
        "\n",
        "\n",
        "        if remove_special_chars:\n",
        "            text = self.remove_special_characters(text, remove_digits)\n",
        "\n",
        "        if remove_punct:\n",
        "            text = self.remove_punctuation(text)\n",
        "\n",
        "        if remove_stops:\n",
        "            text = self.remove_stopwords(text)\n",
        "\n",
        "        if lemmatize:\n",
        "            text = self.lemmatize_text(text)\n",
        "\n",
        "        if stem:\n",
        "            text = self.stem_text(text)\n",
        "\n",
        "        if extra_whitespace:\n",
        "            text = self.remove_extra_whitespace(text)\n",
        "\n",
        "        return text\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the cleaner\n",
        "     cleaner = TextCleaner()\n",
        "     text = \"\"\"\n",
        "    <p>Hey! This is a sample text ðŸ˜Š with HTML tags, emojis, and URLs https://example.com.\n",
        "    It also has some nÃºmeros (numbers) and \"special\" chars!!\n",
        "    Let's see how it's cleaned... We're testing contractions here.</p>\n",
        "    \"\"\"\n",
        "\n",
        "    # Clean the text with default settings\n",
        "     cleaned_text = cleaner.clean_text(text)\n",
        "     print(\"Original text:\", text)\n",
        "     print(\"\\nCleaned text:\", cleaned_text)\n",
        "\n",
        "    # Clean the text with custom settings\n",
        "     custom_cleaned = cleaner.clean_text(\n",
        "        text,\n",
        "        lowercase=True,\n",
        "        remove_html=True,\n",
        "        remove_url=True,\n",
        "        remove_emojis=True,\n",
        "        expand_contractions=True,\n",
        "        remove_accents=True,\n",
        "        remove_special_chars=True,\n",
        "        remove_digits=True,\n",
        "        remove_punct=True,\n",
        "        remove_stops=False,  # Keep stopwords\n",
        "        lemmatize=False,     # No lemmatization\n",
        "        stem=False,          # No stemming\n",
        "        extra_whitespace=True\n",
        "    )\n",
        "     print(\"\\nCustom cleaned text:\", custom_cleaned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i-6rrldkwgW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oyHp4mRcTUE",
        "outputId": "ecae06e0-a5a4-451b-923f-d711292c0dd1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top word associations by PMI score:\n",
            "                 Word 1       Word 2     PMI   NPMI  Co-occurrences\n",
            "0           lokhandwala     shootout  10.684  0.938               8\n",
            "1                africa      eastern  10.491  0.888               6\n",
            "2                africa    mauritius  10.491  0.888               6\n",
            "3                jumped        table  10.491  0.888               6\n",
            "4                others        south  10.421  0.841               4\n",
            "5                awadhi        hindu  10.421  0.841               4\n",
            "6                awadhi     kayastha  10.421  0.841               4\n",
            "7                awadhi       fluent  10.421  0.841               4\n",
            "8            faisalabad       punjab  10.421  0.841               4\n",
            "9            revolution     zindabad  10.421  0.841               4\n",
            "10               phrase   revolution  10.421  0.841               4\n",
            "11             inquilab   revolution  10.421  0.841               4\n",
            "12           infatuated      parwana  10.421  0.841               4\n",
            "13  loverturnedmurderer      parwana  10.421  0.841               4\n",
            "14                    p       ralhan  10.421  0.841               4\n",
            "15                goyle            p  10.421  0.841               4\n",
            "16                 flop          yeh  10.421  0.841               4\n",
            "17                 flop         hogi  10.421  0.841               4\n",
            "18                 flop         quit  10.421  0.841               4\n",
            "19                 bade        miyan  10.421  0.841               4\n",
            "20                chote        miyan  10.421  0.841               4\n",
            "21                   ak       kaante  10.421  0.841               4\n",
            "22                   ak          dev  10.421  0.841               4\n",
            "23                   ak    veerzaara  10.421  0.841               4\n",
            "24          independent       virgin  10.421  0.841               4\n",
            "25          independent         shes  10.421  0.841               4\n",
            "26          independent     sexually  10.421  0.841               4\n",
            "27              embrace  independent  10.421  0.841               4\n",
            "28          desperation         hang  10.421  0.841               4\n",
            "29                  bat         hang  10.421  0.841               4\n",
            "30              billion   equivalent  10.421  0.841               4\n",
            "31              billion    worldwide  10.421  0.841               4\n",
            "32                 sony       viewer  10.421  0.841               4\n",
            "33               panama        paper  10.421  0.841               4\n",
            "34                paper     paradise  10.421  0.841               4\n",
            "35               leaked        paper  10.421  0.841               4\n",
            "36         confidential        paper  10.421  0.841               4\n",
            "37             document        paper  10.421  0.841               4\n",
            "38         constituency          lok  10.421  0.841               4\n",
            "39          beleaguered        clear  10.421  0.841               4\n",
            "40               andhra        clear  10.421  0.841               4\n",
            "41          eradication     goodwill  10.421  0.841               4\n",
            "42             promoted        tiger  10.421  0.841               4\n",
            "43           importance        tiger  10.421  0.841               4\n",
            "44         conservation        tiger  10.421  0.841               4\n",
            "45              helping      promote  10.421  0.841               4\n",
            "46                naina      namrata  10.421  0.841               4\n",
            "47                naina       nilima  10.421  0.841               4\n",
            "48             modelled          wax  10.421  0.841               4\n",
            "49               madame          wax  10.421  0.841               4\n",
            "\n",
            "PMI between 'chote ' and 'miyan': 9.099\n",
            "NPMI between 'chote ' and 'miyan': -1.000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "from nltk.util import ngrams\n",
        "import math\n",
        "from typing import List, Dict, Tuple\n",
        "import pandas as pd\n",
        "\n",
        "class PMICalculator:\n",
        "    def __init__(self, window_size: int = 5):\n",
        "        \"\"\"\n",
        "        Initialize PMI Calculator\n",
        "\n",
        "        Args:\n",
        "            window_size (int): Size of the context window for co-occurrence\n",
        "        \"\"\"\n",
        "        self.window_size = window_size\n",
        "        self.word_freq = Counter()\n",
        "        self.pair_freq = defaultdict(Counter)\n",
        "        self.total_words = 0\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def preprocess_text(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Preprocess text by tokenizing and converting to lowercase\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            List[str]: List of preprocessed tokens\n",
        "        \"\"\"\n",
        "        # Download required NLTK data if not already present\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nltk.download('punkt')\n",
        "\n",
        "        # Tokenize and convert to lowercase\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        return tokens\n",
        "\n",
        "    def compute_co_occurrence(self, tokens: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Compute word frequencies and co-occurrence frequencies\n",
        "\n",
        "        Args:\n",
        "            tokens (List[str]): List of preprocessed tokens\n",
        "        \"\"\"\n",
        "        self.total_words = len(tokens)\n",
        "        self.word_freq.update(tokens)\n",
        "        self.vocab_size = len(self.word_freq)\n",
        "\n",
        "        # Compute co-occurrence frequencies within window\n",
        "        for i, word in enumerate(tokens):\n",
        "            # Define context window\n",
        "            start = max(0, i - self.window_size)\n",
        "            end = min(len(tokens), i + self.window_size + 1)\n",
        "\n",
        "            # Update co-occurrence frequencies\n",
        "            context = tokens[start:i] + tokens[i+1:end]\n",
        "            self.pair_freq[word].update(context)\n",
        "\n",
        "    def calculate_pmi(self, word1: str, word2: str, smooth: bool = True) -> float:\n",
        "        \"\"\"\n",
        "        Calculate PMI for a word pair\n",
        "\n",
        "        Args:\n",
        "            word1 (str): First word\n",
        "            word2 (str): Second word\n",
        "            smooth (bool): Whether to apply smoothing\n",
        "\n",
        "        Returns:\n",
        "            float: PMI value\n",
        "        \"\"\"\n",
        "        # Get individual word probabilities\n",
        "        p_w1 = self.word_freq[word1] / self.total_words\n",
        "        p_w2 = self.word_freq[word2] / self.total_words\n",
        "\n",
        "        # Get joint probability\n",
        "        joint_count = self.pair_freq[word1][word2] + self.pair_freq[word2][word1]\n",
        "        total_pairs = sum(sum(counter.values()) for counter in self.pair_freq.values()) / 2\n",
        "        p_joint = joint_count / total_pairs\n",
        "\n",
        "        # Apply smoothing if requested\n",
        "        if smooth:\n",
        "            # Add-one smoothing\n",
        "            p_w1 = (self.word_freq[word1] + 1) / (self.total_words + self.vocab_size)\n",
        "            p_w2 = (self.word_freq[word2] + 1) / (self.total_words + self.vocab_size)\n",
        "            p_joint = (joint_count + 1) / (total_pairs + self.vocab_size)\n",
        "\n",
        "        # Calculate PMI\n",
        "        if p_joint > 0 and p_w1 > 0 and p_w2 > 0:\n",
        "            pmi = math.log2(p_joint / (p_w1 * p_w2))\n",
        "        else:\n",
        "            pmi = float('-inf')\n",
        "\n",
        "        return pmi\n",
        "\n",
        "    def calculate_npmi(self, word1: str, word2: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculate Normalized PMI for a word pair\n",
        "\n",
        "        Args:\n",
        "            word1 (str): First word\n",
        "            word2 (str): Second word\n",
        "\n",
        "        Returns:\n",
        "            float: NPMI value between -1 and 1\n",
        "        \"\"\"\n",
        "        pmi = self.calculate_pmi(word1, word2)\n",
        "\n",
        "        if pmi == float('-inf'):\n",
        "            return -1.0\n",
        "\n",
        "        # Get joint probability for normalization\n",
        "        joint_count = self.pair_freq[word1][word2] + self.pair_freq[word2][word1]\n",
        "        total_pairs = sum(sum(counter.values()) for counter in self.pair_freq.values()) / 2\n",
        "        p_joint = joint_count / total_pairs\n",
        "\n",
        "        if p_joint > 0:\n",
        "            npmi = pmi / (-math.log2(p_joint))\n",
        "        else:\n",
        "            npmi = -1.0\n",
        "\n",
        "        return npmi\n",
        "\n",
        "    def get_top_collocations(self, n: int = 50) -> List[Tuple[Tuple[str, str], float]]:\n",
        "        \"\"\"\n",
        "        Get top n word pairs by PMI score\n",
        "\n",
        "        Args:\n",
        "            n (int): Number of top collocations to return\n",
        "\n",
        "        Returns:\n",
        "            List[Tuple[Tuple[str, str], float]]: List of word pairs and their PMI scores\n",
        "        \"\"\"\n",
        "        pmi_scores = []\n",
        "\n",
        "        # Calculate PMI for all word pairs\n",
        "        for word1 in self.word_freq:\n",
        "            for word2 in self.pair_freq[word1]:\n",
        "                if word1 < word2:  # Avoid duplicates\n",
        "                    pmi = self.calculate_pmi(word1, word2)\n",
        "                    if pmi != float('-inf'):\n",
        "                        pmi_scores.append(((word1, word2), pmi))\n",
        "\n",
        "        # Sort by PMI score\n",
        "        pmi_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        return pmi_scores[:n]\n",
        "\n",
        "    def analyze_text(self, text: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Analyze text and return PMI statistics\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame containing word pairs and their PMI/NPMI scores\n",
        "        \"\"\"\n",
        "        # Preprocess text and compute frequencies\n",
        "        tokens = self.preprocess_text(text)\n",
        "        self.compute_co_occurrence(tokens)\n",
        "\n",
        "        # Get top collocations\n",
        "        collocations = self.get_top_collocations(n=50)\n",
        "\n",
        "        # Create DataFrame with results\n",
        "        results = []\n",
        "        for (word1, word2), pmi in collocations:\n",
        "            npmi = self.calculate_npmi(word1, word2)\n",
        "            results.append({\n",
        "                'Word 1': word1,\n",
        "                'Word 2': word2,\n",
        "                'PMI': round(pmi, 3),\n",
        "                'NPMI': round(npmi, 3),\n",
        "                'Co-occurrences': self.pair_freq[word1][word2] + self.pair_freq[word2][word1]\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample text\n",
        "    cleaner = TextCleaner()\n",
        "     # Initialize and use PMI calculator\n",
        "    pmi_calc = PMICalculator(window_size=5)\n",
        "    cleanedtext = cleaner.clean_text(fetch_wikipedia_content (\"Amitabh  Bachchan \"))\n",
        "    results_df = pmi_calc.analyze_text(cleanedtext)\n",
        "    print(\"\\nTop word associations by PMI score:\")\n",
        "   # cleanedtext = cleaner.clean_text(results_df )\n",
        "    print(results_df)\n",
        "\n",
        "    # Example of specific word pair analysis\n",
        "    word1, word2 = \"chote \", \"miyan\"\n",
        "    pmi = pmi_calc.calculate_pmi(word1, word2)\n",
        "    npmi = pmi_calc.calculate_npmi(word1, word2)\n",
        "    print(f\"\\nPMI between '{word1}' and '{word2}': {pmi:.3f}\")\n",
        "    print(f\"NPMI between '{word1}' and '{word2}': {npmi:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Tuple, Dict\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "class AdvancedKeywordExtractor:\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        \"\"\"\n",
        "        Initialize the keyword extractor with specified transformer model\n",
        "        \"\"\"\n",
        "        # Download required NLTK data\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "        # Initialize SentenceTransformer model\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def fetch_wikipedia_content(self, topic: str) -> str:\n",
        "        \"\"\"\n",
        "        Fetch content from Wikipedia for a given topic\n",
        "        \"\"\"\n",
        "        try:\n",
        "            page = wikipedia.page(topic)\n",
        "            return page.content\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching Wikipedia content: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def split_into_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into sentences and filter out short ones\n",
        "        \"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        # Filter out very short sentences (likely noise)\n",
        "        return [s for s in sentences if len(s.split()) > 3]\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean and preprocess text\n",
        "        \"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "\n",
        "        # Remove special characters and digits\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Remove stopwords\n",
        "        words = word_tokenize(text)\n",
        "        words = [word for word in words if word not in self.stop_words]\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def generate_ngrams(self, text: str, n_range: Tuple[int, int] = (2, 2)) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate n-grams from text within specified range\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(text)\n",
        "        all_ngrams = []\n",
        "\n",
        "        for n in range(n_range[0], n_range[1] + 1):\n",
        "            n_grams = list(ngrams(tokens, n))\n",
        "            all_ngrams.extend([' '.join(gram) for gram in n_grams])\n",
        "\n",
        "        return all_ngrams\n",
        "\n",
        "    def cluster_sentences(self, sentences: List[str], n_clusters: int = 5) -> Dict[int, List[str]]:\n",
        "        \"\"\"\n",
        "        Cluster sentences using SentenceTransformer embeddings and KMeans\n",
        "        \"\"\"\n",
        "        # Generate embeddings\n",
        "        embeddings = self.model.encode(sentences)\n",
        "\n",
        "        # Perform clustering\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        cluster_assignments = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        # Group sentences by cluster\n",
        "        clusters = {}\n",
        "        for i, cluster in enumerate(cluster_assignments):\n",
        "            if cluster not in clusters:\n",
        "                clusters[cluster] = []\n",
        "            clusters[cluster].append(sentences[i])\n",
        "\n",
        "        return clusters\n",
        "\n",
        "    def max_sum_similarity(self, candidates: List[str], top_n: int = 10) -> List[str]:\n",
        "        \"\"\"\n",
        "        Select diverse keywords using max sum similarity\n",
        "        \"\"\"\n",
        "        # Generate embeddings for candidates\n",
        "        candidate_embeddings = self.model.encode(candidates)\n",
        "\n",
        "        # Calculate similarity matrix\n",
        "        similarities = cosine_similarity(candidate_embeddings)\n",
        "\n",
        "        # Initialize selected indices and similarity sum\n",
        "        selected_indices = []\n",
        "\n",
        "        # Select keywords iteratively\n",
        "        while len(selected_indices) < top_n:\n",
        "            # Calculate similarity sum for each candidate\n",
        "            if not selected_indices:\n",
        "                # For first selection, choose the most central candidate\n",
        "                similarity_sum = similarities.sum(axis=1)\n",
        "                selected_idx = similarity_sum.argmax()\n",
        "            else:\n",
        "                # Calculate marginal similarity gain for remaining candidates\n",
        "                remaining_indices = list(set(range(len(candidates))) - set(selected_indices))\n",
        "                marginal_gains = np.zeros(len(remaining_indices))\n",
        "\n",
        "                for i, idx in enumerate(remaining_indices):\n",
        "                    marginal_gains[i] = sum(similarities[idx, selected_indices])\n",
        "\n",
        "                # Select candidate with minimum marginal similarity (maximum diversity)\n",
        "                selected_idx = remaining_indices[marginal_gains.argmin()]\n",
        "\n",
        "            selected_indices.append(selected_idx)\n",
        "\n",
        "        return [candidates[idx] for idx in selected_indices]\n",
        "\n",
        "    def extract_keywords_from_cluster(self, sentences: List[str], top_n: int = 3) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract keywords from a cluster of sentences\n",
        "        \"\"\"\n",
        "        # Clean and preprocess sentences\n",
        "        cleaned_sentences = [self.clean_text(sent) for sent in sentences]\n",
        "\n",
        "        # Generate n-grams\n",
        "        all_candidates = []\n",
        "        for sent in cleaned_sentences:\n",
        "            all_candidates.extend(self.generate_ngrams(sent))\n",
        "\n",
        "        # Filter candidates by frequency\n",
        "        candidate_freq = Counter(all_candidates)\n",
        "        candidates = [cand for cand, freq in candidate_freq.items() if freq > 1]\n",
        "\n",
        "        if not candidates:\n",
        "            return []\n",
        "\n",
        "        # Select diverse keywords using max sum similarity\n",
        "        return self.max_sum_similarity(candidates, top_n=min(top_n, len(candidates)))\n",
        "\n",
        "    def extract_keywords(self, topic: str, n_clusters: int = 10, keywords_per_cluster: int = 10) -> Dict:\n",
        "        \"\"\"\n",
        "        Main function to extract keywords from a Wikipedia topic using clustering\n",
        "        \"\"\"\n",
        "        # Fetch content\n",
        "        content = self.fetch_wikipedia_content(topic)\n",
        "        if not content:\n",
        "            return {}\n",
        "\n",
        "        # Split into sentences\n",
        "        sentences = self.split_into_sentences(content)\n",
        "\n",
        "        # Cluster sentences\n",
        "        clusters = self.cluster_sentences(sentences, n_clusters=n_clusters)\n",
        "\n",
        "        # Extract keywords from each cluster\n",
        "        keywords_by_cluster = {}\n",
        "        for cluster_id, cluster_sentences in clusters.items():\n",
        "            keywords = self.extract_keywords_from_cluster(\n",
        "                cluster_sentences,\n",
        "                top_n=keywords_per_cluster\n",
        "            )\n",
        "            keywords_by_cluster[f\"Cluster_{cluster_id}\"] = {\n",
        "                'keywords': keywords,\n",
        "                'sample_sentences': cluster_sentences[:5]  # Store sample sentences for context\n",
        "            }\n",
        "\n",
        "        return keywords_by_cluster\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the extractor\n",
        "    extractor = AdvancedKeywordExtractor()\n",
        "\n",
        "    # Extract keywords for a topic\n",
        "    topic = \"Amitabh Bachchan \"\n",
        "    results = extractor.extract_keywords(\n",
        "        topic,\n",
        "        n_clusters=5,\n",
        "        keywords_per_cluster=5)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nKeywords extracted from '{topic}':\")\n",
        "    for cluster, data in results.items():\n",
        "        print(f\"\\n{cluster}:\")\n",
        "        print(\"Keywords:\", \", \".join(data['keywords']))\n",
        "        print(\"\\nSample sentences from this cluster:\")\n",
        "        for i, sent in enumerate(data['sample_sentences'], 1):\n",
        "            print(f\"{i}. {sent[:500]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0uWztm8IYUg",
        "outputId": "0340779d-f888-4159-8638-8a253a2b11d4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Keywords extracted from 'Amitabh Bachchan ':\n",
            "\n",
            "Cluster_1:\n",
            "Keywords: rai bachchan, millennium online, social causes, stage screen, poet harivansh\n",
            "\n",
            "Sample sentences from this cluster:\n",
            "1. Amitabh Bachchan (pronounced [É™mÉªËˆtÌªÉ‘ËbÊ± ËˆbÉ™tËÊƒÉ™n] ; born Amitabh Srivastava; 11 October 1942) is an Indian actor who works in Hindi cinema....\n",
            "2. Bachchan is often hailed as the Shahenshah of Bollywood, Sadi Ke Mahanayak (translated as \"Greatest actor of the century\" in Hindi), Star of the Millennium, or simply Big B....\n",
            "3. Amitabh Bachchan was born in 1942 in Allahabad (now Prayagraj) to the Hindi poet Harivansh Rai Bachchan and his wife, the social activist Teji Bachchan....\n",
            "4. Bachchan also made an appearance in a Hollywood film, The Great Gatsby (2013), in which he played a non-Indian Jewish character....\n",
            "5. In addition to acting, Bachchan has worked as a playback singer, film producer, and television presenter....\n",
            "\n",
            "Cluster_4:\n",
            "Keywords: filmfare award, officer legion, considered one, muqaddar ka, best supporting\n",
            "\n",
            "Sample sentences from this cluster:\n",
            "1. He is often considered one of the greatest, most accomplished and commercially successful actors in the history of Indian cinema....\n",
            "2. With a cinematic career spanning over five decades, he has played pivotal roles in over 200 films....\n",
            "3. He is a recipient of several accolades including six National Film Awards and sixteen Filmfare Awards....\n",
            "4. For Piku, he won his fourth National Film Award for Best Actor, making him the only actor to do so....\n",
            "5. Bachchan has won numerous accolades in his career, including record four National Film Awards in Best Actor category and many awards at international film festivals and award ceremonies....\n",
            "\n",
            "Cluster_2:\n",
            "Keywords: bollywood films, bunty aur, double roles, released may, wife jaya\n",
            "\n",
            "Sample sentences from this cluster:\n",
            "1. His dominance in the Indian film industry during the 1970sâ€“80s led the French director FranÃ§ois Truffaut to describe it as a \"one-man industry\"....\n",
            "2. His film career started in 1969 as a voice narrator in Mrinal Sen's film Bhuvan Shome....\n",
            "3. He first gained popularity in the early-1970s for films, such as Anand, Zanjeer and Roti Kapada Aur Makaan, and achieved greater stardom in later years, being dubbed India's \"Angry Young Man\" for several of his on-screen roles in Hindi films....\n",
            "4. He consistently starred in topâ€“grossing Indian films from the mid-1970s to the 1980s, such as Deewaar, Sholay, Kabhi Kabhie, Hera Pheri, Amar Akbar Anthony, Parvarish, Kasme Vaade, Don, Trishul, Muqaddar Ka Sikandar, Suhaag, Dostana, Naseeb, Laawaris, Namak Halaal, Andhaa Kaanoon, Coolie, Sharaabi and Mard, as well as some of his most acclaimed performances, include Namak Haraam, Abhimaan, Majboor, Mili, Chupke Chupke, Do Anjaane, Kaala Patthar, Shaan, Silsila, Yaarana, Kaalia, Shakti, Aakhree R...\n",
            "5. Since then he starred in several successful and acclaimed films like Kabhi Khushi Kabhie Gham, Aankhen, Baghban, Khakee, Black, Bunty Aur Babli, Sarkar, Kabhi Alvida Naa Kehna, Bhoothnath, Cheeni Kum, Paa, Piku, Pink, Badla, BrahmÄstra: Part One â€“ Shiva and Kalki 2898 AD....\n",
            "\n",
            "Cluster_3:\n",
            "Keywords: university delhi, game show, clear debts, products services, kirori mal\n",
            "\n",
            "Sample sentences from this cluster:\n",
            "1. He was educated at Sherwood College, Nainital, and Kirori Mal College, University of Delhi....\n",
            "2. He has hosted several seasons of the game show Kaun Banega Crorepati, India's version of the game show franchise, Who Wants to Be a Millionaire?....\n",
            "3. He also entered politics for a time in the 1980s....\n",
            "4. Beyond the Indian subcontinent, he acquired a large overseas following of the South Asian diaspora, as well as others, in markets including Africa (South Africa, Eastern Africa, and Mauritius), the Middle East (especially Egypt and the UAE), the United Kingdom, Russia, Central Asia, the Caribbean (Guyana, Suriname, and Trinidad and Tobago), Oceania (Fiji, Australia, and New Zealand), Canada and the United States....\n",
            "5. Harivansh's ancestors came from a village called Babupatti, in the Raniganj tehsil, in the Pratapgarh district, in the present-day state of Uttar Pradesh, in India....\n",
            "\n",
            "Cluster_0:\n",
            "Keywords: film flop, box office, film year\n",
            "\n",
            "Sample sentences from this cluster:\n",
            "1. After taking a break from acting in the 1990s, his resurgence was marked in 2000 with Mohabbatein....\n",
            "2. However, they were struggling to find an actor for the lead \"angry young man\" role; it was turned down by several actors, owing to it going against the \"romantic hero\" image dominant in the industry at the time....\n",
            "3. The film was a huge success and one of the highest-grossing films of that year, breaking Bachchan's dry spell at the box office and making him a star....\n",
            "4. The film opened to excellent response all over the country, eventually taking top spot at the box office that year and emerging an All Time Blockbuster as well as Bachchan's biggest hit at that point of time....\n",
            "5. The film was the highest-grossing film of that year....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEGTYQm2cS2l"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN14gLOeICQ+YVhJJPC7Ip4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
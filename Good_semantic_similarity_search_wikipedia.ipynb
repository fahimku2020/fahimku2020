{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyvlw4g8/441NnCWEfgkUP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahimku2020/fahimku2020/blob/main/Good_semantic_similarity_search_wikipedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jynhKMO0TElR",
        "outputId": "c20b251c-470e-4f0e-ddbb-95930fcf6206"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=2f13e2df93714c7923aa02326c8483c85092fd6f898773dd4503a4a54fb83e3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') #Ensure Punkt Sentence Tokenizer is downloaded\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import wikipedia\n",
        "import re\n",
        "\n",
        "def get_sentences_from_wikipedia(topic, num_sentences=100):\n",
        "    \"\"\"Fetches sentences from Wikipedia and returns a list.\"\"\"\n",
        "    try:\n",
        "        page = wikipedia.page(topic)\n",
        "        text = page.content\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        #Remove sentences with excessive numbers or special characters (Noise reduction)\n",
        "        cleaned_sentences = [s for s in sentences if len(re.findall(r'\\d+', s)) < 5 and len(re.findall(r'[^a-zA-Z0-9\\s]', s)) < 10 ]\n",
        "        return cleaned_sentences[:min(num_sentences, len(cleaned_sentences))]\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        print(f\"Wikipedia page not found for '{topic}'.\")\n",
        "        return []\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        print(f\"Disambiguation error for '{topic}': {e.options}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def semantic_similarity_query(topic, query_sentence):\n",
        "    \"\"\"Builds semantic similarity and performs a query.\"\"\"\n",
        "    sentences = get_sentences_from_wikipedia(topic)\n",
        "    if not sentences:\n",
        "        return []\n",
        "\n",
        "    model = SentenceTransformer('all-mpnet-base-v2')\n",
        "    embeddings = model.encode(sentences)\n",
        "    query_embedding = model.encode(query_sentence)\n",
        "\n",
        "    cosine_scores = util.cos_sim(query_embedding, embeddings)\n",
        "    top_indices = cosine_scores.argsort()[0][-5:].flip(dims=[0]) # Get indices of top 5 most similar\n",
        "\n",
        "    top_similar_sentences = [sentences[i] for i in top_indices]\n",
        "    return top_similar_sentences\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    topic = input(\"Enter the Wikipedia topic (e.g., 'Pakistan'): \")\n",
        "    query = input(\"Enter your query sentence: \")\n",
        "\n",
        "    results = semantic_similarity_query(topic, query)\n",
        "\n",
        "    if results:\n",
        "        print(\"\\nTop 5 most similar sentences:\")\n",
        "        for i, sentence in enumerate(results):\n",
        "            print(f\"{i+1}. {sentence}\")\n",
        "    else:\n",
        "        print(\"No similar sentences found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTxJmOs6Sx7U",
        "outputId": "9785fe78-72b4-4d3a-a7d7-b5d879bef4ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the Wikipedia topic (e.g., 'Pakistan'): Amitabh Bachan \n",
            "Enter your query sentence: Superhit films\n",
            "\n",
            "Top 5 most similar sentences:\n",
            "1. Despite its heavy theme, Kabhi Kabhie went on to become a superhit.\n",
            "2. In 1999, BBC India declared Sholay the \"Film of the Millennium\" and, like Deewaar, it has been cited by Indiatimes Movies as among the Top 25 Must See Bollywood Films.\n",
            "3. He was exceptional, a genius actor who was in films that weren't good.\"\n",
            "4. The film opened to excellent response all over the country, eventually taking top spot at the box office that year and emerging an All Time Blockbuster as well as Bachchan's biggest hit at that point of time.\n",
            "5. The film emerged a huge blockbuster at the box office and earned him another Filmfare nomination for Best Actor.\n"
          ]
        }
      ]
    }
  ]
}